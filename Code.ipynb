{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0d726-17d7-4d3a-bd81-f7606e6b682a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e64b39-96ce-4d15-a070-d9df0f69080a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 519)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:519\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca7fca0c501402894f1926e0dcebf27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f298775f5b274499a01f6512bb2f0dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98cc7745a174135837cf409c24c6915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8469d48b5054c4fb164f7e5a943b665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a80d58735e4b4bbe2db78940bce0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61306ccadd104b308c50967533189cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62747463f90e44148b9922a0eb5e43c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f9acb104074dceb67bfe6cbffb9ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1486fd8e20884cac9017d71a7e067fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b81358101764b1192ea39cf3bcd2476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb86220f3de5455ca4ae5492cc247a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b705a90cf94f8c8e51017d63a1e41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b93f3f8385d48899b8772afbc90b860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 06:58:09,777 - INFO - Detected language: fr with score 0.18\n",
      "2025-03-06 06:58:09,778 - INFO - Detected language: fr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cc6507b73d42158cf9ed944c816c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c206f156d18044d9959c58c8a5790564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6e20c9a90e4d77b28eff5e08884548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c0f353cbe04c1c916d7d19b8c4625d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0c9df7c66744af9207b4c8beb257c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108336266e7b4b8085467d43891c07b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba658e268df4e758cb31a700f7f7c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356aaccfb0b34d868255ea6aac363a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a0b85349de4f1896af2afcf5ee491a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gradio as gr\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import uuid\n",
    "import base64\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "import threading\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except:\n",
    "    logger.warning(\"NLTK download failed. Make sure NLTK is properly installed.\")\n",
    "\n",
    "class LoubbyDocumentation:\n",
    "    \"\"\"Class to handle Loubby platform documentation\"\"\"\n",
    "    \n",
    "    def __init__(self, docs_path=\"loubby_docs.txt\"):\n",
    "        \"\"\"Initialize the documentation handler\"\"\"\n",
    "        self.docs_path = docs_path\n",
    "        self.sections = {}\n",
    "        self.raw_text = self._load_documentation()\n",
    "        self.process_documentation()\n",
    "        \n",
    "    def _load_documentation(self) -> str:\n",
    "        \"\"\"Load documentation from file or create it if it doesn't exist\"\"\"\n",
    "        if not os.path.exists(self.docs_path):\n",
    "            logger.info(f\"Documentation file not found at {self.docs_path}. Creating it...\")\n",
    "            self._create_documentation_file()\n",
    "        \n",
    "        with open(self.docs_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    def _create_documentation_file(self):\n",
    "        \"\"\"Create the documentation file with Loubby platform information\"\"\"\n",
    "        documentation_text = \"\"\"# Loubby Platform Navigation Guide\n",
    "\n",
    "## Recruitment Portal\n",
    "\n",
    "### 1. Profile Setup\n",
    "#### 1.1 Creating Your Account\n",
    "To create your account on Loubby's recruitment portal:\n",
    "1. Navigate to loubby.ai/recruitment\n",
    "2. Click the \"Sign Up\" button in the top right corner\n",
    "3. Enter your email address and create a password\n",
    "4. Verify your email through the link sent to your inbox\n",
    "5. Complete your basic profile information (name, contact details)\n",
    "\n",
    "#### 1.2 Resume Upload\n",
    "To upload your resume to your Loubby profile:\n",
    "1. Log in to your account\n",
    "2. Navigate to \"My Profile\" from the dashboard menu\n",
    "3. Scroll to the \"Resume\" section\n",
    "4. Click \"Upload Resume\" button\n",
    "5. Select your resume file (supported formats: PDF, DOCX, RTF)\n",
    "6. Click \"Save Changes\" to confirm upload\n",
    "7. Verify that your resume appears in the preview section\n",
    "\n",
    "Tip: For best results, use a PDF format that is ATS-friendly\n",
    "\n",
    "#### 1.3 Skills Assessment\n",
    "To complete the skills assessment:\n",
    "1. From the dashboard, select \"Skills Assessment\" tab\n",
    "2. Choose the relevant skill categories for your profile\n",
    "3. For each category, click \"Start Assessment\"\n",
    "4. Complete the assessment questions within the allotted time\n",
    "5. Submit your answers using the \"Submit\" button\n",
    "6. View your results and skill ratings on completion\n",
    "\n",
    "### 2. Job Search and Application\n",
    "\n",
    "#### 2.1 Searching for Positions\n",
    "To search for open positions on Loubby:\n",
    "1. Click the \"Jobs\" tab in the main navigation\n",
    "2. Use the search bar to enter keywords, job titles, or company names\n",
    "3. Filter results using the sidebar options:\n",
    "   - Location (remote, on-site, hybrid)\n",
    "   - Experience level\n",
    "   - Industry\n",
    "   - Salary range\n",
    "   - Posted date\n",
    "4. Sort results by relevance, date, or salary using the dropdown menu\n",
    "5. Save searches by clicking \"Save This Search\" for future reference\n",
    "\n",
    "#### 2.2 Applying for a Position\n",
    "To apply for a position:\n",
    "1. From the job listing, click \"Apply Now\"\n",
    "2. Review your profile information for completeness\n",
    "3. Complete any additional application questions\n",
    "4. Upload any position-specific documents if requested\n",
    "5. Review all information for accuracy\n",
    "6. Click \"Submit Application\"\n",
    "7. Confirm submission in the popup window\n",
    "\n",
    "#### 2.3 Tracking Applications\n",
    "To track your submitted applications:\n",
    "1. Navigate to \"My Applications\" in the dashboard\n",
    "2. View all applications and their current status\n",
    "3. Filter by status (submitted, in review, interview scheduled, etc.)\n",
    "4. Click on any application to view details\n",
    "5. Check for any action items or requested information\n",
    "\n",
    "### 3. Interview Process\n",
    "\n",
    "#### 3.1 Scheduling Interviews\n",
    "When you receive an interview request:\n",
    "1. You'll receive an email notification and an in-platform notification\n",
    "2. Navigate to \"My Applications\" in the dashboard\n",
    "3. Find the application with the interview request\n",
    "4. Click on \"Schedule Interview\"\n",
    "5. Select from available time slots in the calendar interface\n",
    "6. Confirm your selection\n",
    "7. Add to your personal calendar using the calendar integration options\n",
    "\n",
    "#### 3.2 Virtual Interview Room\n",
    "To access the virtual interview room:\n",
    "1. 15 minutes before your scheduled interview, log in to Loubby\n",
    "2. Go to \"My Applications\" in the dashboard\n",
    "3. Find the relevant application\n",
    "4. Click \"Join Interview\" (button becomes active 15 minutes before start time)\n",
    "5. Test your audio and video in the pre-meeting room\n",
    "6. Click \"Join Now\" when ready\n",
    "7. If you experience technical difficulties, use the support chat in the bottom right\n",
    "\n",
    "## Employee Portal\n",
    "\n",
    "### 1. Onboarding\n",
    "\n",
    "#### 1.1 Accessing the Employee Portal\n",
    "After accepting a job offer:\n",
    "1. You'll receive an email with login credentials for the employee portal\n",
    "2. Navigate to loubby.ai/employee\n",
    "3. Enter your credentials from the email\n",
    "4. Set up two-factor authentication if prompted\n",
    "5. Create a new password that meets security requirements\n",
    "\n",
    "#### 1.2 Completing Onboarding Documents\n",
    "To complete necessary onboarding paperwork:\n",
    "1. From the employee dashboard, select \"Onboarding\" tab\n",
    "2. View the list of required documents\n",
    "3. Click \"Complete\" next to each document\n",
    "4. Fill out all required fields\n",
    "5. Use the e-signature tool where required\n",
    "6. Click \"Submit\" for each completed document\n",
    "7. Track completion progress in the onboarding checklist\n",
    "\n",
    "#### 1.3 Training Modules\n",
    "To access and complete training modules:\n",
    "1. Navigate to \"Training\" in the main menu\n",
    "2. View assigned training modules and their deadlines\n",
    "3. Click on a module to begin\n",
    "4. Complete all sections of the module\n",
    "5. Take the assessment quiz at the end\n",
    "6. Achieve the minimum required score to mark as complete\n",
    "7. View your training transcript in the \"My Learning\" section\n",
    "\n",
    "### 2. Course Synchronization\n",
    "\n",
    "#### 2.1 Connecting External Courses\n",
    "To sync external courses with the Loubby platform:\n",
    "1. From the dashboard, navigate to \"Education & Courses\"\n",
    "2. Select \"Connect External Course\" button\n",
    "3. Choose from the list of supported learning platforms\n",
    "4. Log in to your account on the selected platform\n",
    "5. Authorize the connection when prompted\n",
    "6. Select which courses to sync with Loubby\n",
    "7. Click \"Confirm\" to complete the integration\n",
    "\n",
    "#### 2.2 Viewing Course Progress\n",
    "To monitor your progress across all courses:\n",
    "1. Go to \"My Learning\" in the main navigation\n",
    "2. View the dashboard showing all courses and completion percentages\n",
    "3. Filter by course status, start date, or provider\n",
    "4. Click on any course to view detailed progress\n",
    "5. See upcoming assignments and deadlines\n",
    "6. View instructor feedback on completed assignments\n",
    "\n",
    "#### 2.3 Submitting Assignments\n",
    "To submit assignments through Loubby:\n",
    "1. Navigate to \"My Learning\" > \"Assignments\"\n",
    "2. Find the assignment you want to submit\n",
    "3. Click \"Submit Assignment\"\n",
    "4. Upload required files or enter text submission\n",
    "5. Add any comments for the instructor\n",
    "6. Review your submission\n",
    "7. Click \"Submit\" to finalize\n",
    "\n",
    "### 3. Performance Tracking\n",
    "\n",
    "#### 3.1 Setting Goals\n",
    "To set and track professional goals:\n",
    "1. From the employee dashboard, select \"Performance\" tab\n",
    "2. Click \"My Goals\" in the submenu\n",
    "3. Select \"Add New Goal\"\n",
    "4. Define your goal with the SMART criteria\n",
    "5. Set target date and key milestones\n",
    "6. Link to relevant skills or competencies\n",
    "7. Submit for manager approval\n",
    "\n",
    "#### 3.2 Feedback and Reviews\n",
    "To access performance feedback:\n",
    "1. Navigate to \"Performance\" > \"Feedback\"\n",
    "2. View all feedback received from peers and supervisors\n",
    "3. Filter by date, project, or feedback type\n",
    "4. For formal reviews, select \"Reviews\" from the submenu\n",
    "5. View scheduled and past review cycles\n",
    "6. Complete self-assessment when prompted\n",
    "7. Review manager feedback and ratings\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "#### Password Reset\n",
    "If you need to reset your password:\n",
    "1. On the login page, click \"Forgot Password\"\n",
    "2. Enter the email associated your account\n",
    "3. Check your email for a reset link\n",
    "4. Click the link and follow instructions to create a new password\n",
    "5. If you don't receive the email, check spam folder or click \"Resend Email\"\n",
    "\n",
    "#### Browser Compatibility\n",
    "Loubby works best with:\n",
    "- Chrome (version 90+)\n",
    "- Firefox (version 88+)\n",
    "- Safari (version 14+)\n",
    "- Edge (version 90+)\n",
    "\n",
    "If experiencing issues:\n",
    "1. Update your browser to the latest version\n",
    "2. Clear browser cache and cookies\n",
    "3. Disable extensions that might interfere\n",
    "4. Try an alternate supported browser\n",
    "\n",
    "#### Mobile Access\n",
    "To access Loubby on mobile devices:\n",
    "1. Use the responsive web version on any mobile browser\n",
    "2. Download the Loubby mobile app from App Store or Google Play\n",
    "3. Log in with the same credentials as the web version\n",
    "4. Enable notifications for interview alerts\n",
    "5. Note that some advanced features may require desktop access\n",
    "\n",
    "### Getting Help\n",
    "\n",
    "#### Live Support\n",
    "To access live support:\n",
    "1. Click the \"Help\" icon in the bottom right corner\n",
    "2. Select \"Chat with Support\"\n",
    "3. Describe your issue in detail\n",
    "4. Support is available Monday-Friday, 9am-6pm EST\n",
    "\n",
    "#### Knowledge Base\n",
    "To search the help documentation:\n",
    "1. Click \"Help\" in the main navigation\n",
    "2. Use the search bar to find relevant articles\n",
    "3. Browse by category using the sidebar\n",
    "4. Rate articles to help improve documentation\n",
    "\"\"\"\n",
    "        with open(self.docs_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(documentation_text)\n",
    "        \n",
    "        logger.info(f\"Documentation file created at {self.docs_path}\")\n",
    "    \n",
    "    def process_documentation(self):\n",
    "        \"\"\"Process the documentation into sections\"\"\"\n",
    "        # Split by main sections using regex\n",
    "        section_pattern = r'(#+)\\s+(.+)'\n",
    "        current_section = {\"title\": \"Root\", \"content\": \"\", \"level\": 0, \"subsections\": []}\n",
    "        section_stack = [current_section]\n",
    "        \n",
    "        for line in self.raw_text.split('\\n'):\n",
    "            match = re.match(section_pattern, line)\n",
    "            if match:\n",
    "                level = len(match.group(1))\n",
    "                title = match.group(2).strip()\n",
    "                \n",
    "                new_section = {\"title\": title, \"content\": \"\", \"level\": level, \"subsections\": []}\n",
    "                \n",
    "                # Pop from stack until we find the parent section\n",
    "                while level <= section_stack[-1][\"level\"]:\n",
    "                    section_stack.pop()\n",
    "                \n",
    "                # Add new section as subsection of parent\n",
    "                section_stack[-1][\"subsections\"].append(new_section)\n",
    "                \n",
    "                # Push new section to stack\n",
    "                section_stack.append(new_section)\n",
    "            else:\n",
    "                # Add content to current section\n",
    "                if section_stack[-1][\"content\"]:\n",
    "                    section_stack[-1][\"content\"] += \"\\n\" + line\n",
    "                else:\n",
    "                    section_stack[-1][\"content\"] += line\n",
    "        \n",
    "        self.sections = section_stack[0]\n",
    "        logger.info(\"Documentation processed successfully\")\n",
    "\n",
    "    def get_flattened_sections(self) -> List[Dict]:\n",
    "        \"\"\"Flatten the hierarchical sections into a list with full paths\"\"\"\n",
    "        flattened = []\n",
    "        \n",
    "        def traverse(section, path=\"\"):\n",
    "            current_path = f\"{path} > {section['title']}\" if path else section['title']\n",
    "            \n",
    "            # Add current section\n",
    "            if section['content'].strip():\n",
    "                flattened.append({\n",
    "                    \"path\": current_path,\n",
    "                    \"title\": section['title'],\n",
    "                    \"content\": section['content'],\n",
    "                    \"level\": section['level']\n",
    "                })\n",
    "            \n",
    "            # Process subsections\n",
    "            for subsection in section['subsections']:\n",
    "                traverse(subsection, current_path)\n",
    "        \n",
    "        traverse(self.sections)\n",
    "        return flattened\n",
    "\n",
    "class DocumentChunker:\n",
    "    \"\"\"Class to chunk documentation for efficient retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=200, chunk_overlap=50):\n",
    "        \"\"\"Initialize the document chunker\"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "    \n",
    "    def chunk_document(self, sections: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Split sections into smaller chunks for retrieval\"\"\"\n",
    "        chunked_sections = []\n",
    "        \n",
    "        for section in sections:\n",
    "            # Combine section info with content for context\n",
    "            full_text = f\"{section['path']}\\n\\n{section['content']}\"\n",
    "            \n",
    "            # Split text into chunks\n",
    "            chunks = self.text_splitter.split_text(full_text)\n",
    "            \n",
    "            # Create dictionary entries for each chunk\n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                chunked_sections.append({\n",
    "                    \"id\": f\"{section['path']}_{i}\",\n",
    "                    \"path\": section['path'],\n",
    "                    \"title\": section['title'],\n",
    "                    \"chunk_index\": i,\n",
    "                    \"content\": chunk_text,\n",
    "                    \"level\": section['level']\n",
    "                })\n",
    "        \n",
    "        logger.info(f\"Created {len(chunked_sections)} chunks from {len(sections)} sections\")\n",
    "        return chunked_sections\n",
    "\n",
    "class VectorDatabase:\n",
    "    \"\"\"Class to manage vector embeddings and similarity search\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the vector database\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        self.vectorstore = None\n",
    "        self.chunk_data = None\n",
    "    \n",
    "    def create_vectors(self, chunked_sections: List[Dict]):\n",
    "        \"\"\"Create vector embeddings for document chunks\"\"\"\n",
    "        # Extract texts and metadata\n",
    "        texts = [chunk[\"content\"] for chunk in chunked_sections]\n",
    "        metadatas = [{\n",
    "            \"id\": chunk[\"id\"],\n",
    "            \"path\": chunk[\"path\"],\n",
    "            \"title\": chunk[\"title\"],\n",
    "            \"chunk_index\": chunk[\"chunk_index\"],\n",
    "            \"level\": chunk[\"level\"]\n",
    "        } for chunk in chunked_sections]\n",
    "        \n",
    "        # Create FAISS index\n",
    "        self.vectorstore = FAISS.from_texts(\n",
    "            texts=texts,\n",
    "            embedding=self.embeddings,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "        \n",
    "        # Store the original chunk data for reference\n",
    "        self.chunk_data = {chunk[\"id\"]: chunk for chunk in chunked_sections]}\n",
    "        \n",
    "        logger.info(f\"Created vector database with {len(texts)} documents\")\n",
    "        return self.vectorstore\n",
    "    \n",
    "    def save_vectors(self, path=\"loubby_vectors\"):\n",
    "        \"\"\"Save vector database to disk\"\"\"\n",
    "        if self.vectorstore:\n",
    "            self.vectorstore.save_local(path)\n",
    "            # Save chunk data separately\n",
    "            with open(f\"{path}_chunks.json\", 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.chunk_data, f)\n",
    "            logger.info(f\"Vector database saved to {path}\")\n",
    "        else:\n",
    "            logger.error(\"No vector database to save\")\n",
    "    \n",
    "    def load_vectors(self, path=\"loubby_vectors\"):\n",
    "        \"\"\"Load vector database from disk\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                self.vectorstore = FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "                # Load chunk data\n",
    "                with open(f\"{path}_chunks.json\", 'r', encoding='utf-8') as f:\n",
    "                    self.chunk_data = json.load(f)\n",
    "                logger.info(f\"Vector database loaded from {path}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading vector database: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            logger.warning(f\"No vector database found at {path}\")\n",
    "            return False\n",
    "    \n",
    "    def similarity_search(self, query: str, k=3) -> List[Dict]:\n",
    "        \"\"\"Search for most similar chunks to the query\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            logger.error(\"Vector database not initialized\")\n",
    "            return []\n",
    "        \n",
    "        results = self.vectorstore.similarity_search_with_score(query, k=k)\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        for doc, score in results:\n",
    "            chunk_id = doc.metadata[\"id\"]\n",
    "            formatted_results.append({\n",
    "                \"id\": chunk_id,\n",
    "                \"path\": doc.metadata[\"path\"],\n",
    "                \"title\": doc.metadata[\"title\"],\n",
    "                \"content\": doc.page_content,\n",
    "                \"similarity\": float(score),\n",
    "                \"chunk_index\": doc.metadata[\"chunk_index\"],\n",
    "                \"level\": doc.metadata[\"level\"]\n",
    "            })\n",
    "        \n",
    "        return formatted_results\n",
    "\n",
    "class FeedbackSystem:\n",
    "    \"\"\"System to collect and use user feedback to improve responses\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path=\"loubby_feedback.db\"):\n",
    "        \"\"\"Initialize the feedback system\"\"\"\n",
    "        self.db_path = db_path\n",
    "        self._init_database()\n",
    "    \n",
    "    def _init_database(self):\n",
    "        \"\"\"Initialize the SQLite database for feedback\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Create feedback table if it doesn't exist\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS feedback (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    query TEXT NOT NULL,\n",
    "                    response TEXT NOT NULL,\n",
    "                    rating INTEGER NOT NULL,\n",
    "                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "                    user_comment TEXT\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            logger.info(f\"Feedback database initialized at {self.db_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing feedback database: {e}\")\n",
    "    \n",
    "    def record_feedback(self, query: str, response: str, rating: int, user_comment: str = \"\"):\n",
    "        \"\"\"Record user feedback on a response\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\n",
    "                \"INSERT INTO feedback (query, response, rating, user_comment) VALUES (?, ?, ?, ?)\",\n",
    "                (query, response, rating, user_comment)\n",
    "            )\n",
    "            \n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            logger.info(f\"Feedback recorded: Rating {rating} for query '{query[:30]}...'\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recording feedback: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_historical_feedback(self, limit=100) -> List[Dict]:\n",
    "        \"\"\"Get historical feedback data\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            conn.row_factory = sqlite3.Row\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\n",
    "                \"SELECT * FROM feedback ORDER BY timestamp DESC LIMIT ?\",\n",
    "                (limit,)\n",
    "            )\n",
    "            \n",
    "            # Convert to list of dictionaries\n",
    "            results = [dict(row) for row in cursor.fetchall()]\n",
    "            \n",
    "            conn.close()\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching historical feedback: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_feedback_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about feedback ratings\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Get average rating\n",
    "            cursor.execute(\"SELECT AVG(rrating) FROM feedback\")\n",
    "            avg_rating = cursor.fetchone()[0] or 0\n",
    "            \n",
    "            # Get rating distribution\n",
    "            cursor.execute(\n",
    "                \"SELECT rating, COUNT(*) FROM feedback GROUP BY rating ORDER BY rating\"\n",
    "            )\n",
    "            rating_counts = dict(cursor.fetchall())\n",
    "            \n",
    "            # Get total feedback count\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM feedback\")\n",
    "            total_count = cursor.fetchone()[0]\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            return {\n",
    "                \"average_rating\": round(avg_rating, 2),\n",
    "                \"rating_distribution\": rating_counts,\n",
    "                \"total_count\": total_count\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching feedback statistics: {e}\")\n",
    "            return {\n",
    "                \"average_rating\": 0,\n",
    "                \"rating_distribution\": {},\n",
    "                \"total_count\": 0\n",
    "            }\n",
    "\n",
    "class LanguageProcessor:\n",
    "    \"\"\"Class for handling natural language processing tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=\"google/flan-t5-base\"):\n",
    "        \"\"\"Initialize the language processor\"\"\"\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "            logger.info(f\"Language processor initialized with model {model_path}\")\n",
    "            \n",
    "            # Load language detection model\n",
    "            self.lang_detector = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "            self.language_map = {\n",
    "                \"en\": \"English\",\n",
    "                \"es\": \"Spanish\", \n",
    "                \"fr\": \"French\",\n",
    "                \"de\": \"German\",\n",
    "                \"zh\": \"Chinese\",\n",
    "                \"ja\": \"Japanese\",\n",
    "                \"ru\": \"Russian\",\n",
    "                \"ar\": \"Arabic\",\n",
    "                \"hi\": \"Hindi\",\n",
    "                \"pt\": \"Portuguese\",\n",
    "                \"it\": \"Italian\",\n",
    "                \"ko\": \"Korean\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing language processor: {e}\")\n",
    "            self.tokenizer = None\n",
    "            self.model = None\n",
    "    \n",
    "    def detect_language(self, text: str) -> str:\n",
    "        \"\"\"Detect the language of input text using a multilingual sentence transformer\"\"\"\n",
    "        if not text.strip():\n",
    "            return \"en\"  # Default to English for empty text\n",
    "        \n",
    "        try:\n",
    "            # Get embeddings for the text\n",
    "            text_embedding = self.lang_detector.encode(text)\n",
    "            \n",
    "            # Compare with embeddings of language markers\n",
    "            language_scores = {}\n",
    "            for lang, lang_name in self.language_map.items():\n",
    "                # Create a simple sentence in the target language\n",
    "                lang_marker = f\"This is a sentence in {lang_name}.\"\n",
    "                lang_embedding = self.lang_detector.encode(lang_marker)\n",
    "                \n",
    "                # Calculate cosine similarity\n",
    "                similarity = cosine_similarity([text_embedding], [lang_embedding])[0][0]\n",
    "                language_scores[lang] = similarity\n",
    "            \n",
    "            # Return the language with the highest similarity\n",
    "            detected_lang = max(language_scores.items(), key=lambda x: x[1])[0]\n",
    "            logger.info(f\"Detected language: {detected_lang} with score {language_scores[detected_lang]:.2f}\")\n",
    "            \n",
    "            return detected_lang\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error detecting language: {e}\")\n",
    "            return \"en\"  # Default to English on error\n",
    "    \n",
    "    def translate_text(self, text: str, target_language: str = \"en\") -> str:\n",
    "        \"\"\"Simple translation function (placeholder for actual translation)\"\"\"\n",
    "        # Note: In a real implementation, you would integrate with a translation API\n",
    "        # Since we're omitting external API dependencies for this cloud environment, \n",
    "        # we'll return the original text with a note\n",
    "        \n",
    "        detected_language = self.detect_language(text)\n",
    "        \n",
    "        # Skip translation if already in target language\n",
    "        if detected_language == target_language:\n",
    "            return text\n",
    "        \n",
    "        logger.info(f\"Translation requested from {detected_language} to {target_language}\")\n",
    "        \n",
    "        # In a real implementation, you would call a translation service here\n",
    "        # For now, just return the original text\n",
    "        return text\n",
    "    \n",
    "    def generate_response(self, query: str, retrieved_chunks: List[Dict]) -> str:\n",
    "        \"\"\"Generate a comprehensive response based on the retrieved chunks\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            return \"Language model not properly initialized.\"\n",
    "        \n",
    "        try:\n",
    "            # Create a context from retrieved chunks\n",
    "            context = \"\\n\\n\".join([chunk[\"content\"] for chunk in retrieved_chunks])\n",
    "            \n",
    "            # Build a prompt\n",
    "            prompt = f\"\"\"\n",
    "            CONTEXT: {context}\n",
    "            \n",
    "            QUERY: {query}\n",
    "            \n",
    "            Based on the above context, provide a clear and concise answer to the query about navigating the Loubby platform.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Generate response\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "            outputs = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=512,\n",
    "                min_length=64,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response: {e}\")\n",
    "            \n",
    "            # Fallback response method\n",
    "            most_relevant_chunk = retrieved_chunks[0] if retrieved_chunks else None\n",
    "            if most_relevant_chunk:\n",
    "                return f\"Based on the Loubby documentation, here's guidance on your question:\\n\\n{most_relevant_chunk['content']}\"\n",
    "            else:\n",
    "                return \"I'm unable to generate a specific response at the moment. Please try rephrasing your question about the Loubby platform.\"\n",
    "\n",
    "class VoiceInterface:\n",
    "    \"\"\"Class to handle voice input and output using Gradio components\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the voice interface\"\"\"\n",
    "        logger.info(\"Voice interface initialized using Gradio components\")\n",
    "    \n",
    "    def process_audio(self, audio):\n",
    "        \"\"\"Process audio input and return text\"\"\"\n",
    "        try:\n",
    "            if audio is None:\n",
    "                return \"\"\n",
    "            \n",
    "            # Use Gradio's built-in audio processing\n",
    "            recognizer = sr.Recognizer()\n",
    "            with sr.AudioFile(audio) as source:\n",
    "                audio_data = recognizer.record(source)\n",
    "                text = recognizer.recognize_google(audio_data)\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing audio: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def generate_audio(self, text, language=\"en\"):\n",
    "        \"\"\"Generate audio from text with language support\"\"\"\n",
    "        try:\n",
    "            if not text.strip():\n",
    "                return None\n",
    "            \n",
    "            tts = gTTS(text=text, lang=language)\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False) as f:\n",
    "                tts.save(f.name)\n",
    "                return f.name\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating audio: {e}\")\n",
    "            return None\n",
    "\n",
    "class VideoInterface:\n",
    "    \"\"\"Class to handle video communication in cloud environment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the video interface with dummy video for cloud environments\"\"\"\n",
    "        self.is_running = False\n",
    "        self.thread = None\n",
    "        self.frame = None\n",
    "        self.user_face_detected = False\n",
    "        self.animation_frame = 0\n",
    "        self.animation_speed = 0.15\n",
    "        self.last_animation_time = time.time()\n",
    "        \n",
    "        # Create avatars since we can't use real camera\n",
    "        self.user_avatar = self._create_user_avatar()\n",
    "        self.assistant_avatar = self._create_assistant_avatar()\n",
    "        \n",
    "        logger.info(\"Video interface initialized with virtual avatars for cloud environment\")\n",
    "    \n",
    "    def _create_user_avatar(self):\n",
    "        \"\"\"Create a simple avatar for the user in cloud environments\"\"\"\n",
    "        avatar = np.zeros((240, 320, 3), dtype=np.uint8)\n",
    "        # Set background color\n",
    "        avatar[:, :] = (50, 50, 50)\n",
    "        # Draw a simple circle face\n",
    "        cv2.circle(avatar, (160, 120), 80, (100, 100, 220), -1)\n",
    "        # Draw eyes\n",
    "        cv2.circle(avatar, (130, 100), 15, (255, 255, 255), -1)\n",
    "        cv2.circle(avatar, (190, 100), 15, (255, 255, 255), -1)\n",
    "        cv2.circle(avatar, (130, 100), 7, (0, 0, 0), -1)\n",
    "        cv2.circle(avatar, (190, 100), 7, (0, 0, 0), -1)\n",
    "        # Draw mouth\n",
    "        cv2.ellipse(avatar, (160, 140), (40, 20), 0, 0, 180, (0, 0, 0), 3)\n",
    "        \n",
    "        # Add text label\n",
    "        cv2.putText(avatar, \"User (virtual)\", (80, 210), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "        cv2.putText(avatar, \"Cloud Environment\", (70, 230), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        \n",
    "        return avatar\n",
    "    \n",
    "    def _create_assistant_avatar(self):\n",
    "        \"\"\"Create a simple avatar for the assistant\"\"\"\n",
    "        avatar = np.zeros((240, 320, 3), dtype=np.uint8)\n",
    "        # Set background color\n",
    "        avatar[:, :] = (30, 30, 30)\n",
    "        # Draw a simple circle face\n",
    "        cv2.circle(avatar, (160, 120), 80, (70, 130, 180), -1)\n",
    "        # Draw eyes\n",
    "        cv2.circle(avatar, (130, 100), 15, (255, 255, 255), -1)\n",
    "        cv2.circle(avatar, (190, 100), 15, (255, 255, 255), -1)\n",
    "        cv2.circle(avatar, (130, 100), 7, (0, 0, 0), -1)\n",
    "        cv2.circle(avatar, (190, 100), 7, (0, 0, 0), -1)\n",
    "        # Draw mouth (will be animated)\n",
    "        cv2.ellipse(avatar, (160, 140), (40, 15), 0, 0, 180, (0, 0, 0), 3)\n",
    "        \n",
    "        # Add text label\n",
    "        cv2.putText(avatar, \"Loubby Assistant\", (100, 210), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "        \n",
    "        return avatar\n",
    "    \n",
    "    def _update_avatar_animation(self):\n",
    "        \"\"\"Update the avatar's animation\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_animation_time > self.animation_speed:\n",
    "            # Create a copy of the base avatar\n",
    "            animated_avatar = self.assistant_avatar.copy()\n",
    "            \n",
    "            # Animate the mouth based on speaking state\n",
    "            mouth_height = 10 + 10 * np.sin(self.animation_frame)\n",
    "            cv2.ellipse(animated_avatar, (160, 140), (40, int(mouth_height)), 0, 0, 180, (0, 0, 0), 3)\n",
    "            \n",
    "            self.animation_frame += 0.5\n",
    "            self.last_animation_time = current_time\n",
    "            \n",
    "            return animated_avatar\n",
    "        return self.assistant_avatar.copy()\n",
    "    \n",
    "    def start_video(self):\n",
    "        \"\"\"Start the virtual video session\"\"\"\n",
    "        if self.is_running:\n",
    "            return True\n",
    "        \n",
    "        self.is_running = True\n",
    "        \n",
    "        # Start animation thread\n",
    "        self.thread = threading.Thread(target=self._animate_avatars)\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "        \n",
    "        logger.info(\"Virtual video avatars started for cloud environment\")\n",
    "        return True\n",
    "    \n",
    "    def stop_video(self):\n",
    "        \"\"\"Stop the virtual video session\"\"\"\n",
    "        self.is_running = False\n",
    "        \n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=1.0)\n",
    "            self.thread = None\n",
    "        \n",
    "        logger.info(\"Virtual video avatars stopped\")\n",
    "        return True\n",
    "    \n",
    "    def _animate_avatars(self):\n",
    "        \"\"\"Animate the avatars in a continuous loop\"\"\"\n",
    "        while self.is_running:\n",
    "            # Update the assistant avatar animation\n",
    "            self._update_avatar_animation()\n",
    "            time.sleep(0.05)\n",
    "    \n",
    "    def get_user_frame(self):\n",
    "        \"\"\"Get the user's avatar frame\"\"\"\n",
    "        return self.user_avatar\n",
    "    \n",
    "    def get_assistant_frame(self):\n",
    "        \"\"\"Get the assistant's animated avatar frame\"\"\"\n",
    "        return self._update_avatar_animation()\n",
    "    \n",
    "    def generate_combined_frame(self):\n",
    "        \"\"\"Generate a combined frame with user and assistant avatars\"\"\"\n",
    "        user_frame = self.get_user_frame()\n",
    "        assistant_frame = self.get_assistant_frame()\n",
    "        \n",
    "        # Create a combined frame (side by side)\n",
    "        combined = np.zeros((240, 640, 3), dtype=np.uint8)\n",
    "        combined[:, :320] = user_frame\n",
    "        combined[:, 320:] = assistant_frame\n",
    "        \n",
    "        # Add a separating line\n",
    "        cv2.line(combined, (320, 0), (320, 240), (100, 100, 100), 2)\n",
    "        \n",
    "        # Add a status indicator showing this is a cloud environment\n",
    "        cv2.rectangle(combined, (180, 10), (460, 30), (0, 0, 100), -1)\n",
    "        cv2.putText(combined, \"Cloud Environment - Virtual Video\", (190, 25), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def process_video_call(self, text_message):\n",
    "        \"\"\"Process a simulated video call with the assistant\"\"\"\n",
    "        if not self.is_running:\n",
    "            return \"Please start the video call first.\", self._create_assistant_avatar()\n",
    "        \n",
    "        # Generate a combined frame with user and assistant\n",
    "        combined_frame = self.generate_combined_frame()\n",
    "        \n",
    "        if not text_message:\n",
    "            return \"I'm here and ready to help! Type a message to ask about the Loubby platform.\", combined_frame\n",
    "        \n",
    "        return f\"Processing: '{text_message}'\", combined_frame\n",
    "\n",
    "class LoubbyAssistant:\n",
    "    \"\"\"Main assistant class that integrates all components\"\"\"\n",
    "    \n",
    "    def __init__(self, load_existing=True):\n",
    "        \"\"\"Initialize the Loubby Assistant\"\"\"\n",
    "        self.documentation = LoubbyDocumentation()\n",
    "        self.chunker = DocumentChunker(chunk_size=300, chunk_overlap=100)\n",
    "        self.vector_db = VectorDatabase()\n",
    "        self.feedback_system = FeedbackSystem()\n",
    "        self.language_processor = LanguageProcessor()\n",
    "        self.voice_interface = VoiceInterface()\n",
    "        self.video_interface = VideoInterface()\n",
    "        \n",
    "        # Initialize vector database\n",
    "        if load_existing and self.vector_db.load_vectors():\n",
    "            logger.info(\"Using existing vector database\")\n",
    "        else:\n",
    "            logger.info(\"Creating new vector database\")\n",
    "            sections = self.documentation.get_flattened_sections()\n",
    "            chunks = self.chunker.chunk_document(sections)\n",
    "            self.vector_db.create_vectors(chunks)\n",
    "            self.vector_db.save_vectors()\n",
    "        \n",
    "        # Track conversation history\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # Map of language codes to full names for internal use\n",
    "        self.language_map = {\n",
    "            \"en\": \"English\",\n",
    "            \"es\": \"Spanish\", \n",
    "            \"fr\": \"French\",\n",
    "            \"de\": \"German\",\n",
    "            \"zh\": \"Chinese\",\n",
    "            \"ja\": \"Japanese\",\n",
    "            \"ru\": \"Russian\",\n",
    "            \"ar\": \"Arabic\",\n",
    "            \"hi\": \"Hindi\",\n",
    "            \"pt\": \"Portuguese\",\n",
    "            \"it\": \"Italian\",\n",
    "            \"ko\": \"Korean\"\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Loubby Assistant initialized and ready\")\n",
    "    \n",
    "    async def process_query(self, query: str, use_voice=False) -> Dict:\n",
    "        \"\"\"Process a user query and generate a response with language detection\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Detect language (using our improved detection method)\n",
    "        detected_language = self.language_processor.detect_language(query)\n",
    "        logger.info(f\"Detected language: {detected_language}\")\n",
    "        \n",
    "        translated_query = query\n",
    "        \n",
    "        # Translate to English for processing if not already in English\n",
    "        if detected_language != \"en\":\n",
    "            translated_query = self.language_processor.translate_text(query, target_language=\"en\")\n",
    "            logger.info(f\"Translated query from {detected_language} to English: {translated_query}\")\n",
    "        \n",
    "        # Search for relevant chunks\n",
    "        retrieved_chunks = self.vector_db.similarity_search(translated_query, k=3)\n",
    "        \n",
    "        # Generate response in English\n",
    "        response_en = \"\"\n",
    "        if retrieved_chunks:\n",
    "            response_en = self.language_processor.generate_response(translated_query, retrieved_chunks)\n",
    "        else:\n",
    "            response_en = \"I'm sorry, I couldn't find specific information about that in the Loubby documentation. Could you please rephrase your question about using the Loubby platform?\"\n",
    "        \n",
    "        # Translate response back to original language if needed\n",
    "        response = response_en\n",
    "        if detected_language != \"en\":\n",
    "            try:\n",
    "               response = self.language_processor.translate_text(response_en, target_language=detected_language)\n",
    "               logger.info(f\"Translated response to {detected_language}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error Translating response to {detected_language}: {e}\")\n",
    "                response=response_en\n",
    "                \n",
    "        # Add sources information\n",
    "        sources = []\n",
    "        for chunk in retrieved_chunks:\n",
    "            if chunk[\"path\"] not in [s[\"path\"] for s in sources]:\n",
    "                sources.append({\n",
    "                    \"path\": chunk[\"path\"],\n",
    "                    \"title\": chunk[\"title\"]\n",
    "                })\n",
    "        \n",
    "        # Use voice interface if requested\n",
    "        audio_file = None\n",
    "        if use_voice:\n",
    "            audio_file = self.voice_interface.generate_audio(response, language=detected_language)\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"sources\": sources,\n",
    "            \"language\": detected_language\n",
    "        })\n",
    "        \n",
    "        # Calculate processing time\n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"sources\": sources,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"detected_language\": detected_language,\n",
    "            \"audio_file\": audio_file,\n",
    "            \"language\": detected_language\n",
    "        }\n",
    "    \n",
    "    async def voice_query(self, audio):\n",
    "        \"\"\"Handle a voice query from the user\"\"\"\n",
    "        # Process audio input\n",
    "        query = self.voice_interface.process_audio(audio)\n",
    "        \n",
    "        if not query:\n",
    "            response = {\n",
    "                \"query\": \"\",\n",
    "                \"response\": \"I didn't hear anything. Please try speaking again.\",\n",
    "                \"sources\": [],\n",
    "                \"processing_time\": 0,\n",
    "                \"detected_language\": \"en\",\n",
    "                \"audio_file\": None,\n",
    "                \"language\": \"en\"\n",
    "            }\n",
    "            return response[\"query\"], response[\"response\"], \", \".join([s for s in []]), response[\"audio_file\"]\n",
    "        \n",
    "        # Process the query\n",
    "        result = await self.process_query(query, use_voice=True)\n",
    "        return result[\"query\"], result[\"response\"], \", \".join([s[\"path\"] for s in result[\"sources\"]]), result[\"audio_file\"]\n",
    "    \n",
    "    async def video_chat_query(self, message):\n",
    "        \"\"\"Handle a text query during video call\"\"\"\n",
    "        if not message.strip():\n",
    "            return \"Please type a message to continue the conversation.\"\n",
    "            \n",
    "        # Process the query as normal text query\n",
    "        result = await self.process_query(message)\n",
    "        return result[\"response\"]\n",
    "    \n",
    "    def record_feedback(self, query: str, response: str, rating: int, comment: str = \"\") -> bool:\n",
    "        \"\"\"Record user feedback on a response\"\"\"\n",
    "        return self.feedback_system.record_feedback(query, response, rating, comment)\n",
    "    \n",
    "    def get_feedback_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about feedback ratings\"\"\"\n",
    "        return self.feedback_system.get_feedback_statistics()\n",
    "    \n",
    "    def get_conversation_history(self) -> List[Dict]:\n",
    "        \"\"\"Get the conversation history\"\"\"\n",
    "        return self.conversation_history\n",
    "\n",
    "# Gradio Interface\n",
    "def create_interface(assistant):\n",
    "    \"\"\"Create a Gradio interface for the Loubby Assistant\"\"\"\n",
    "    \n",
    "    async def process_text_query(query, voice_output=False):\n",
    "        \"\"\"Process a text query and return the response with automatic language detection\"\"\"\n",
    "        if not query.strip():\n",
    "            return \"Please enter a question about the Loubby platform.\", \"\", None, \"No query detected\"\n",
    "            \n",
    "        result = await assistant.process_query(query, use_voice=voice_output)\n",
    "        detected_language = result[\"detected_language\"]\n",
    "        language_name = assistant.language_map.get(detected_language, detected_language)\n",
    "        \n",
    "        return result[\"response\"], \", \".join([s[\"path\"] for s in result[\"sources\"]]), result[\"audio_file\"], f\"Detected language: {language_name}\"\n",
    "    \n",
    "    async def process_voice_query(audio):\n",
    "        \"\"\"Process a voice query and return the response\"\"\"\n",
    "        query, response, sources, audio_file = await assistant.voice_query(audio)\n",
    "        \n",
    "        # Detect language for display\n",
    "        if query:\n",
    "            detected_language = assistant.language_processor.detect_language(query)\n",
    "            language_name = assistant.language_map.get(detected_language, detected_language)\n",
    "            language_info = f\"Detected language: {language_name}\"\n",
    "        else:\n",
    "            language_info = \"No speech detected\"\n",
    "            \n",
    "        return query, response, sources, audio_file, language_info\n",
    "    \n",
    "    async def submit_feedback(query, response, rating, comment):\n",
    "        \"\"\"Submit feedback for a response\"\"\"\n",
    "        success = assistant.record_feedback(query, response, rating, comment)\n",
    "        return \"Feedback submitted successfully. Thank you!\" if success else \"Failed to submit feedback. Please try again.\"\n",
    "    \n",
    "    async def show_statistics():\n",
    "        \"\"\"Show feedback statistics\"\"\"\n",
    "        stats = assistant.get_feedback_statistics()\n",
    "        return f\"\"\"\n",
    "        Feedback Statistics:\n",
    "        - Average Rating: {stats['average_rating']} / 5\n",
    "        - Total Feedback Count: {stats['total_count']}\n",
    "        - Rating Distribution: {stats['rating_distribution']}\n",
    "        \"\"\"\n",
    "    \n",
    "    # Video chat functionality with virtual avatars for cloud environment\n",
    "    def start_video_call():\n",
    "        \"\"\"Start video capture for video chat\"\"\"\n",
    "        success = assistant.video_interface.start_video()\n",
    "        status = \"Video call started with virtual avatars (cloud environment mode).\" if success else \"Failed to start video call.\"\n",
    "        return status\n",
    "    \n",
    "    def end_video_call():\n",
    "        \"\"\"End video capture for video chat\"\"\"\n",
    "        success = assistant.video_interface.stop_video()\n",
    "        status = \"Video call ended.\" if success else \"Error ending video call.\"\n",
    "        return status\n",
    "    \n",
    "    async def video_chat_message(message, chat_history):\n",
    "        \"\"\"Handle chat messages during video calls\"\"\"\n",
    "        if not message.strip():\n",
    "            processed_message = \"I'm here to help with Loubby platform navigation.\"\n",
    "        else:\n",
    "            processed_message = await assistant.video_chat_query(message)\n",
    "        \n",
    "        # Get combined video frame\n",
    "        response_text, video_frame = assistant.video_interface.process_video_call(message)\n",
    "        \n",
    "        # Update chat history\n",
    "        chat_history = chat_history or []\n",
    "        chat_history.append((message, processed_message))\n",
    "        \n",
    "        return chat_history, gr.update(value=\"\"), video_frame\n",
    "    \n",
    "    def video_frame_update():\n",
    "        \"\"\"Update the video frame regularly\"\"\"\n",
    "        if not assistant.video_interface.is_running:\n",
    "            blank_frame = np.zeros((240, 640, 3), dtype=np.uint8)\n",
    "            cv2.putText(blank_frame, \"Start video call to see avatars\", (180, 120), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            return blank_frame\n",
    "            \n",
    "        return assistant.video_interface.generate_combined_frame()\n",
    "    \n",
    "    # Custom CSS for styling\n",
    "    custom_css = \"\"\"\n",
    "    .gradio-container {\n",
    "        background: linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab);\n",
    "        background_size: 400% 400%;\n",
    "        animation: gradient 15s ease infinite;\n",
    "    }\n",
    "    @keyframes gradient {\n",
    "        0% { background-position: 0% 50%; }\n",
    "        50% { background-position: 100% 50%; }\n",
    "        100% { background-position: 0% 50%; }\n",
    "    }\n",
    "    .gradio-interface {\n",
    "        background: rgba(255, 255, 255, 0.9);\n",
    "        border-radius: 15px;\n",
    "        padding: 20px;\n",
    "        box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);\n",
    "        backdrop-filter: blur(5px);\n",
    "        -webkit-backdrop-filter: blur(5px);\n",
    "    }\n",
    "    .video-interface {\n",
    "        background: rgba(255, 255, 255, 0.95);\n",
    "        border-radius: 15px;\n",
    "        padding: 15px;\n",
    "        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);\n",
    "    }\n",
    "    .gradio-feedback {\n",
    "        background: rgba(245, 245, 245, 0.9);\n",
    "        border-radius: 10px;\n",
    "        padding: 15px;\n",
    "    }\n",
    "    .video-container {\n",
    "        display: flex;\n",
    "        flex-direction: column;\n",
    "        width: 100%;\n",
    "    }\n",
    "    .chat-container {\n",
    "        display: flex;\n",
    "        flex-direction: column;\n",
    "        flex-grow: 1;\n",
    "        min-height: 300px;\n",
    "        overflow-y: auto;\n",
    "        padding: 10px;\n",
    "        background: rgba(240, 240, 240, 0.7);\n",
    "        border-radius: 8px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "    .language-info {\n",
    "        background: rgba(255, 255, 255, 0.7);\n",
    "        padding: 5px 10px;\n",
    "        border-radius: 5px;\n",
    "        display: inline-block;\n",
    "        margin-top: 5px;\n",
    "        font-size: 0.9em;\n",
    "        color: #555;\n",
    "    }\n",
    "    .cloud-notice {\n",
    "        background: rgba(255, 200, 0, 0.2);\n",
    "        border: 1px solid #FFB800;\n",
    "        padding: 8px;\n",
    "        border-radius: 5px;\n",
    "        margin-bottom: 10px;\n",
    "        font-size: 0.9em;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"Loubby Navigation Assistant\", css=custom_css) as interface:\n",
    "        gr.Markdown(\"\"\"\n",
    "        <div style=\"text-align: center;\">\n",
    "            <h1 style=\"color: white;\">Loubby AI Navigation Assistant</h1>\n",
    "            <p style=\"color: white;\">Your personal guide to navigating the Loubby platform</p>\n",
    "        </div>\n",
    "        <div class=\"cloud-notice\">\n",
    "            <strong>Cloud Environment Notice:</strong> Running in cloud mode with virtual video avatars instead of camera input.\n",
    "        </div>\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            with gr.Tab(\"Text Interface\", elem_classes=\"gradio-interface\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=3):\n",
    "                        text_input = gr.Textbox(label=\"Your Question\", placeholder=\"How do I upload my resume?\", lines=2)\n",
    "                        \n",
    "                        language_info = gr.Markdown(\"Language will be detected automatically\", elem_classes=\"language-info\")\n",
    "                        voice_checkbox = gr.Checkbox(label=\"Enable Voice Output\", value=False)\n",
    "                        text_submit = gr.Button(\"Ask\", variant=\"primary\")\n",
    "                    \n",
    "                    with gr.Column(scale=4):\n",
    "                        text_output = gr.Textbox(label=\"Response\", lines=8)\n",
    "                        sources_output = gr.Textbox(label=\"Sources\")\n",
    "                        audio_output = gr.Audio(label=\"Voice Response\", visible=False)\n",
    "                \n",
    "                text_submit.click(\n",
    "                    process_text_query,\n",
    "                    inputs=[text_input, voice_checkbox],\n",
    "                    outputs=[text_output, sources_output, audio_output, language_info]\n",
    "                )\n",
    "                \n",
    "                # Also allow pressing Enter to submit\n",
    "                text_input.submit(\n",
    "                    process_text_query,\n",
    "                    inputs=[text_input, voice_checkbox],\n",
    "                    outputs=[text_output, sources_output, audio_output, language_info]\n",
    "                )\n",
    "                \n",
    "                voice_checkbox.change(\n",
    "                    lambda x: gr.Audio.update(visible=x),\n",
    "                    inputs=voice_checkbox,\n",
    "                    outputs=audio_output\n",
    "                )\n",
    "            \n",
    "            with gr.Tab(\"Voice Interface\", elem_classes=\"gradio-interface\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        voice_info = gr.Markdown(\"Language will be detected automatically\", elem_classes=\"language-info\")\n",
    "                        voice_input = gr.Audio(label=\"Speak Your Question\", type=\"filepath\")\n",
    "                        voice_button = gr.Button(\"Submit\", variant=\"primary\")\n",
    "                    with gr.Column():\n",
    "                        voice_query_display = gr.Textbox(label=\"Your Question\")\n",
    "                        voice_response = gr.Textbox(label=\"Response\", lines=8)\n",
    "                        voice_sources = gr.Textbox(label=\"Sources\")\n",
    "                        voice_audio_output = gr.Audio(label=\"Voice Response\")\n",
    "                \n",
    "                voice_button.click(\n",
    "                    process_voice_query,\n",
    "                    inputs=[voice_input],\n",
    "                    outputs=[voice_query_display, voice_response, voice_sources, voice_audio_output, voice_info]\n",
    "                )\n",
    "            \n",
    "            with gr.Tab(\"Video Communication\", elem_classes=\"video-interface\"):\n",
    "                # Initialize chat history state\n",
    "                chat_history = gr.State([])\n",
    "                \n",
    "                with gr.Row():\n",
    "                    # Video feed display with combined user and assistant views\n",
    "                    video_display = gr.Image(label=\"Video Call\", elem_id=\"video-display\")\n",
    "                    \n",
    "                    # Controls and chat area\n",
    "                    with gr.Column():\n",
    "                        video_call_status = gr.Markdown(\"Click 'Start Video Call' to begin communicating with the assistant.\")\n",
    "                        \n",
    "                        # Start and end call buttons\n",
    "                        with gr.Row():\n",
    "                            start_button = gr.Button(\"Start Video Call\", variant=\"primary\")\n",
    "                            end_button = gr.Button(\"End Video Call\", variant=\"secondary\")\n",
    "                        \n",
    "                        # Chat display\n",
    "                        chat_display = gr.Chatbot(label=\"Conversation\", elem_classes=\"chat-container\")\n",
    "                        \n",
    "                        # Chat input during video call\n",
    "                        with gr.Row():\n",
    "                            video_chat_input = gr.Textbox(\n",
    "                                label=\"Type a message\", \n",
    "                                placeholder=\"Ask a question about the Loubby platform...\",\n",
    "                                lines=2\n",
    "                            )\n",
    "                            video_chat_button = gr.Button(\"Send\", variant=\"primary\")\n",
    "                \n",
    "                # Connect everything\n",
    "                start_button.click(\n",
    "                    start_video_call,\n",
    "                    inputs=None,\n",
    "                    outputs=video_call_status\n",
    "                )\n",
    "                \n",
    "                end_button.click(\n",
    "                    end_video_call,\n",
    "                    inputs=None,\n",
    "                    outputs=video_call_status\n",
    "                )\n",
    "                \n",
    "                # Handle sending messages in video chat\n",
    "                video_chat_button.click(\n",
    "                    video_chat_message,\n",
    "                    inputs=[video_chat_input, chat_history],\n",
    "                    outputs=[chat_display, video_chat_input, video_display]\n",
    "                ).then(\n",
    "                    lambda x: x,\n",
    "                    inputs=chat_display,\n",
    "                    outputs=chat_history\n",
    "                )\n",
    "                \n",
    "                # Also allow pressing Enter to send message\n",
    "                video_chat_input.submit(\n",
    "                    video_chat_message,\n",
    "                    inputs=[video_chat_input, chat_history],\n",
    "                    outputs=[chat_display, video_chat_input, video_display]\n",
    "                ).then(\n",
    "                    lambda x: x,\n",
    "                    inputs=chat_display,\n",
    "                    outputs=chat_history\n",
    "                )\n",
    "                \n",
    "                # Update video frame every 0.5 seconds\n",
    "                #gr.on(\n",
    "                    #\"load\",\n",
    "                    #lambda: gr.update(value=video_frame_update()),\n",
    "                    #None,\n",
    "                    #video_display,\n",
    "                    #every=0.5\n",
    "                #)\n",
    "            \n",
    "            with gr.Tab(\"Feedback\", elem_classes=\"gradio-feedback\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        feedback_query = gr.Textbox(label=\"Question\", lines=2)\n",
    "                        feedback_response = gr.Textbox(label=\"Response\", lines=4)\n",
    "                        feedback_rating = gr.Slider(minimum=1, maximum=5, step=1, value=3, label=\"Rating\")\n",
    "                        feedback_comment = gr.Textbox(label=\"Comment (Optional)\", lines=2)\n",
    "                        feedback_submit = gr.Button(\"Submit Feedback\", variant=\"primary\")\n",
    "                        feedback_result = gr.Textbox(label=\"Result\")\n",
    "                    with gr.Column():\n",
    "                        stats_button = gr.Button(\"Show Feedback Statistics\", variant=\"secondary\")\n",
    "                        stats_display = gr.Textbox(label=\"Statistics\")\n",
    "                \n",
    "                feedback_submit.click(\n",
    "                    submit_feedback,\n",
    "                    inputs=[feedback_query, feedback_response, feedback_rating, feedback_comment],\n",
    "                    outputs=[feedback_result]\n",
    "                )\n",
    "                \n",
    "                stats_button.click(\n",
    "                    show_statistics,\n",
    "                    inputs=None,\n",
    "                    outputs=[stats_display]\n",
    "                )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "def run_main():\n",
    "    \"\"\"Function to run the main assistant\"\"\"\n",
    "    # Initialize the assistant\n",
    "    assistant = LoubbyAssistant()\n",
    "    \n",
    "    # Create and launch the Gradio interface\n",
    "    interface = create_interface(assistant)\n",
    "    interface.launch(share=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e899cde0-5e7b-419c-bb99-fca95b095369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:45:22.519042: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-06 00:45:22.533972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741221922.553807  181441 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741221922.559885  181441 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-06 00:45:22.579163: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-06 00:45:26,654 - INFO - Documentation processed successfully\n",
      "/tmp/ipykernel_181441/38530420.py:397: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
      "2025-03-06 00:45:26,658 - INFO - Use pytorch device_name: cpu\n",
      "2025-03-06 00:45:26,658 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-03-06 00:45:26,950 - INFO - Feedback database initialized at loubby_feedback.db\n",
      "2025-03-06 00:45:27,871 - INFO - Language processor initialized with model google/flan-t5-base\n",
      "2025-03-06 00:45:27,874 - INFO - Use pytorch device_name: cpu\n",
      "2025-03-06 00:45:27,875 - INFO - Load pretrained SentenceTransformer: sentence-transformers/distiluse-base-multilingual-cased-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b684bb59a047c18cc68439439eb0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/341 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16b45daf01a4e93ba68359eb412a409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b224efe1dba4da4bc6fb9425c2eb5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984b941d55b44fde8698634297a1e012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d473e7c092415c9b998f0a89d58910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b684c0598cff41a8a92ad2c5783bc269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/539M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db53f9a354214088a2be6e4ef62e05a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7417b0e089604a329ccc141a69ee8095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f93b2123fe44e708e4f74dd6483bb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53196e81981b437f9e95f12905e55d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc13f55f11a41b690729140fcd3e14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad24c85bd2804283ac3c3efc9a82b13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a275bdd1c73a4ee0ab293a3356839242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6313461c6704036b3339d9e5a5c6596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:45:31,551 - INFO - Voice interface initialized using Gradio components\n",
      "2025-03-06 00:45:31,553 - INFO - Video interface initialized with virtual avatars for cloud environment\n",
      "2025-03-06 00:45:31,555 - INFO - Loading faiss with AVX512-SPR support.\n",
      "2025-03-06 00:45:31,555 - INFO - Could not load library with AVX512-SPR support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512_spr'\")\n",
      "2025-03-06 00:45:31,556 - INFO - Loading faiss with AVX512 support.\n",
      "2025-03-06 00:45:31,570 - INFO - Successfully loaded faiss with AVX512 support.\n",
      "2025-03-06 00:45:31,574 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes.\n",
      "2025-03-06 00:45:31,575 - INFO - Vector database loaded from loubby_vectors\n",
      "2025-03-06 00:45:31,576 - INFO - Using existing vector database\n",
      "2025-03-06 00:45:31,576 - INFO - Loubby Assistant initialized and ready\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/components/chatbot.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "2025-03-06 00:45:31,830 - INFO - HTTP Request: GET http://127.0.0.1:7863/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 00:45:31,847 - INFO - HTTP Request: HEAD http://127.0.0.1:7863/ \"HTTP/1.1 200 OK\"\n",
      "2025-03-06 00:45:31,872 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:45:32,102 - INFO - HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://3cf710ffdd008333d3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:45:32,745 - INFO - HTTP Request: HEAD https://3cf710ffdd008333d3.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3cf710ffdd008333d3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e3b41f62bc46b7b32659123b0c2a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590c4c9f23814a9a9f663b91d855eb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab5c89735174c08a2fc92d0ed8ab7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683e786825af495a88de903c10e9458f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f72555e79a2437090968e6c6870d61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a57e4a0aa54601ac62c703adafd15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360b52f9623b47d9a36e99d75fd36257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72e66b068704757b5dcfae1e5f79c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b047db6eb640deb6a8ac32830091ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4928117f3384920bca00aade68a87fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d472855eb64c31a2f67b9a9fc396dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496acd2b4d1f433ea5a1f77c4827848d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86aeb6d7eb584c3c90fa137cdfe06db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:46:32,008 - INFO - Detected language: it with score 0.28\n",
      "2025-03-06 00:46:32,009 - INFO - Detected language: it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7985f290d946b3a527a399b57a0172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee8c3ffbff748f387df3a5571dc085a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9152515c1bc04eb88a285b2e4e8e8763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53ce150d96b43b1a4f67f267f121b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdb3f9e57c348a18d306f9fddd21e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0f7dd79b374a349f61796370e72adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b373cb51bc8f42529624cde857b41106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb81d23b32b431a959eefa010c7ff70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7b074e34d84453a0c5666c33a42678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d968e822844b77be8fe5dffe4bfcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14984275ae44e0ea5ecd41829c9d3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53e8a6e54de4810acafe88a1769f3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e3e48085da34e3e9f905284df0e8e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:46:32,444 - INFO - Detected language: it with score 0.28\n",
      "2025-03-06 00:46:32,446 - INFO - Translation requested from it to en\n",
      "2025-03-06 00:46:32,446 - INFO - Translated query from it to English: zan je ina zuwa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a79381714f404189035929cb186efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8ac4f2be2e467786d2cba6f9b923f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9bc1ecc80e34885b20432fd31e99ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729b34f8e5f84248a705993eb1651a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085d2aacfff745f1a130322370a08b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbf4e77d89043188c02800a9b355b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63ecbe1f48e4671a151401eeb5d25d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba00366c3f024fefaf174dd79b73bb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7668f66b8b2d4e3091ae782d6c24242f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ddc519d8f94fc9b9922a9d5c2bce5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72e9efb967941848232dc9d1ea07429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76f1d1279b9417cb90f78a19a7b2d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375ace4d884f4735b002f3e2eab1458d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:46:38,588 - INFO - Detected language: it with score 0.13\n",
      "2025-03-06 00:46:38,588 - INFO - Translated response to it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be94a80d93844242b8027d41a1cfdc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c0023ae6f644b58547a0cb8b0e3be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88db7d002c3746d8bc87f676ed32031c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3bd70b5fd24136bc603ceca87b15e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3718cfdf367446e185fa09dcd17d93b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ab04cbb0d64eab8b2438719f6aa428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916acb004e62431db0e655a7a9481349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89312aebcf534b8e866a496755957b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbc70a9236149b18f90ac6ab6ddd788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24225dd5f9e4404a0c5379bf4ca3b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a718c98ae96b47cbba21faaa90c66577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0202c104ce0426f82c166bd9fb73281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b902e229806043dc96e2d4f419b816e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:48:31,318 - INFO - Detected language: fr with score 0.18\n",
      "2025-03-06 00:48:31,321 - INFO - Detected language: fr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c807b4f173d4bfc9b9056a8c85189e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde428f4a71a451cb0a77df8e2eb2224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed46f9dc9564068996dbf028250903a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d6d26a9b864d3bbdd60aac0d25d1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3024a64dd364312adfdeca2f886c77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65675e591741464389203bf4a54b7bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4fa35e1f584090ab0679876ccacb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254627de04714216bdc92335cede0fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cc68bb565e4657b147c211745a85a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9642ae26920417fbe4d029a7e449fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13eb05bfa2164c1bbdf7f75d59945c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5655def556742d9acf166091ad3fe64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66138f63ee2453b9f92dd16f066ed9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:48:31,828 - INFO - Detected language: fr with score 0.18\n",
      "2025-03-06 00:48:31,828 - INFO - Translation requested from fr to en\n",
      "2025-03-06 00:48:31,829 - INFO - Translated query from fr to English: Bien sr ! Je peux vous aider  prparer un entretien en franais. De quel type d'entretien s'agit-il ? Est-ce un entretien d'embauche, un entretien acadmique, ou autre chose ? Avez-vous des questions spcifiques ou des thmes sur lesquels vous souhaitez vous concentrer ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184b7a7a004641c08e8204b274f65a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dc3182bd504631bdae5314755fb32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec0f73c5d96495facba8f36a33e290c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc8d6c9c3cb4bafbff6987e75b73e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5987f207a6ca4967a8ecd81ab7e3fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9c1593dbac4a718db90c7587d3d196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4880ee5ff24c407099503bd1b4380210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a148e784684cd3b706a2e2a3a4f0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ddc83795a046b9a043dfb62e5ce78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74beea2766aa4b4196d083ab2090dd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48d89a6cc614a8da80328d3ef8e18b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a621f11d1aca47a687771f0be1f68515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740d1eff4128480ea53630dd69063b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:48:37,641 - INFO - Detected language: fr with score 0.25\n",
      "2025-03-06 00:48:37,641 - INFO - Translated response to fr\n",
      "2025-03-06 00:49:25,838 - INFO - Virtual video avatars started for cloud environment\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1155279be80421cbf48b3e3081d22dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e025a9cd1fa44cb78ce38a818b259a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0910b515141e46a587169dad0dec79f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17f9cc260154a5a8e13653519405477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a6adda40e1464d87d30df8d67bcf1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14a51339fdf4aa283e23b8d24eb6553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac589b481d844fa4863a24053775d083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73168ea766c645dc9980c1d68a89435e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402ece631f0648a7b362ca6769e20cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8997545b80549d389f32dd64b2d9eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2797d7f2e1e42478d3b94a8e8915838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5491d517d3494c64af4d63550d0c80b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f477156e1a34fa9a8141223eb88bab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:49:38,730 - INFO - Detected language: fr with score 0.18\n",
      "2025-03-06 00:49:38,731 - INFO - Detected language: fr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ca86b004374d7d9c89a0740909a4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242720c791874c5d975179342a39c34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db3fbffd63a479ba2767b5dd00e3762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cd34150bb64163b44e92ad5644e913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53490e66c8c04f28b27150a90bcb2336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d350e4afb0254acc9b0359721fc0eafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c792238fa7f04a38bb7b7f6d6c8d0513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02025ea0236342088590d14e4e24bf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364ee7ff0e024913a827b36afb0801d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fddcc9cbfb4b52a6146f4a44b0efae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466565fbcef04ce9bb5e2571b41ad0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35e5b2e78d24e60af2e84b3ecf74812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb8e7b893f74925aa8eeea5078e9fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:49:39,288 - INFO - Detected language: fr with score 0.18\n",
      "2025-03-06 00:49:39,289 - INFO - Translation requested from fr to en\n",
      "2025-03-06 00:49:39,291 - INFO - Translated query from fr to English: Bien sr ! Je peux vous aider  prparer un entretien en franais. De quel type d'entretien s'agit-il ? Est-ce un entretien d'embauche, un entretien acadmique, ou autre chose ? Avez-vous des questions spcifiques ou des thmes sur lesquels vous souhaitez vous concentrer ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df60fbb69b745208c9acfb9134942bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8004485c6cee46b2ba48692aee752108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5767ad7e5bae4a66abb8aa5c9320acf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccc88ee7b9d4abbad7275324efa2229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e819478a4c430d82d372010821faec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe06e8a12fb45a3b71b9fe1d5515bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002a0d24d05e4b5997dbac8c4778db22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c69c16271db45f5b08b7ebe7a37590e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fbc47024cb4979bddd71e6d018a8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e21266bda964607ac5ee812db5ff843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30604624e04a4d98a2f8fb35d01829b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f95754742948cdba535ba690a46c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900633e54b47466781622814a600f304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:49:47,362 - INFO - Detected language: fr with score 0.25\n",
      "2025-03-06 00:49:47,363 - INFO - Translated response to fr\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef64ae7b40cb4f43ae4361455fb029e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d72c1603d7c4f9b8e1f1a2faa109bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fbb12dabee47eb83989eed054e24f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8933676a54458aa8816d77d845361a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91754b70d2a9495eb00a0c4baa640b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694b6f90b6f7474c843be0d074dcbe8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e44ef64b74b44699b3497df3ee7cc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab8ab70a8e444dc9fce8a1e5b0fcd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91bf001d1d043f18cabcdaaa490b3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ac4624cdd14f7a9c2c664b9454b6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511d9cb055f74819a6c39d8ddcf01b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240e71878c0e40419ee9c81e6e8d8177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186d2e4a68d548df9a8092969ee33397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:51:54,563 - INFO - Detected language: de with score 0.18\n",
      "2025-03-06 00:51:54,564 - INFO - Detected language: de\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce2bb776e6949078a379e14746878be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ac7b5f97d84b89926aef636848fa32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b231e55140c348a68b7ae919e6161f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0ba19fff014202bfdfbe1c7a506a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38246d3af7b6442b87a2c4fcd720f268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63be046bed124108b6ddeaf60337c6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747ab143df8f4cc3839da0a4ff8ae4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35837125ef7041bea0d34e66c09f2208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb73965e9ee24cacaebf28b58493dfc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f666e595a7be445f873d5b6be67bdc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649f3ac609e34ff48919eadc1353490f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c526760480a4eadbb4580c3d8bf57a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c6767b55d24a7ba21d31aa4f69abde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:51:55,008 - INFO - Detected language: de with score 0.18\n",
      "2025-03-06 00:51:55,009 - INFO - Translation requested from de to en\n",
      "2025-03-06 00:51:55,009 - INFO - Translated query from de to English: Natrlich! Ich kann Ihnen helfen, sich auf ein Interview auf Deutsch vorzubereiten. Handelt es sich um ein Vorstellungsgesprch, ein akademisches Interview oder etwas anderes? Haben Sie spezifische Fragen oder Themen, auf die Sie sich konzentrieren mchten?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829005f8a6de442ca21f283c2a0d28e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1fe341e57b408aa0d6437b434416b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad88130c07cb4e9a83447f49ac985aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82fa9dfa7664c8a84bdb07a035ffdb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb5a751d6cb4bc8ad6b447392d04c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737cf6c856814333bdc9a1369db80d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61202408e7d14a9f90021dff9762c2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6923f8ac5ba64853b51a0413f52247e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3d42536fa64dacac1034b214a28608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e823b176e63249e8a905ff5164900581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca59e67cacf4d44bb937a01292748a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989d15c9e7ba47f489f7fe40cd21d462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58ab98a11dd4e3db49e363dd67a2b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:52:00,539 - INFO - Detected language: de with score 0.30\n",
      "2025-03-06 00:52:00,540 - INFO - Translated response to de\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/blocks.py\", line 2108, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/blocks.py\", line 1655, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_181441/38530420.py\", line 1238, in <lambda>\n",
      "    lambda x: gr.Audio.update(visible=x),\n",
      "AttributeError: type object 'Audio' has no attribute 'update'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ff2086e08749cc80b7ed2e939a6be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ab39a005f7477cb7932e924e7d27b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53a445519b141168f7d7aaf65250f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38358ea3b90a4062a55302d60fac98a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e507e263779468d810b494d618e2e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d34d056dc274b2cb4cb27e321ba8b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12829768f934ef7bef1c2e6092bd147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6ba57da09647169b79a8fb1fcd9ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5dde7449d2485fbc1b15fd1767ab3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4d0ff9dd73453690eafa8e787d659d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9e3b2eac864cabaced8e0ab5c02dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13b36e3c3de4000ae0f448dcc520827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b98fa5572ca4184b5c5d330aa698aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:52:40,610 - INFO - Detected language: de with score 0.18\n",
      "2025-03-06 00:52:40,610 - INFO - Detected language: de\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262dfb627dea4498af8afcc3082765b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c144d3761340be8145da00b020dea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6567ebd988945b6887510486ba7351b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241f947588c144859939d719702996c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010d6e50b0a844b4ad4eb978141965fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb2e01e2a504f37a0a3ab50adeecaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b90f3eddd14e1f8848f9d4a2adba35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86324113555f4350be70375663fb59a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42a26f3fb2f4012a86c26fa3dbdd203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6319538168f643bbaf571984c4b12b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab010d8022142e2af619e1a13776d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f974b5a4de449383da0a9ee847d0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5257c4cde145495791d2be9058761510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:52:41,154 - INFO - Detected language: de with score 0.18\n",
      "2025-03-06 00:52:41,154 - INFO - Translation requested from de to en\n",
      "2025-03-06 00:52:41,154 - INFO - Translated query from de to English: Natrlich! Ich kann Ihnen helfen, sich auf ein Interview auf Deutsch vorzubereiten. Handelt es sich um ein Vorstellungsgesprch, ein akademisches Interview oder etwas anderes? Haben Sie spezifische Fragen oder Themen, auf die Sie sich konzentrieren mchten?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d898827ecd41a4a60c0afa0c9012a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fa6aacc39449c7a3b9a067847ebe5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecbbd3009974df98d6a12e962bb8283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761ef7ae0fbf4d6fa01ad5ccfc959185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0418971a034a28aa3b78bc72feb7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab58eca8d0464a079b3be807b54eae24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc2b7a795a24b8ebfb309807ba5743f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59618dec29f347258fe56d911e0f2a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede2880a627c45c9815593d7ae70e7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88378b51e9d4b84943508b310dbdc46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a4e76b62284a8f87da75a25df34259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a407e03419e449fb754f19b7d3923a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0ea1764aea464fb958672ba2ca60a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:52:48,113 - INFO - Detected language: de with score 0.30\n",
      "2025-03-06 00:52:48,114 - INFO - Translated response to de\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36674f9d9a474e56bac47bdee57b2b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fefb6696fa74043b6c522aa55eb02d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f97dfca402f4ee680e8113661176c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bd5c85530a4a2e9667d2242300536a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d21d9cedd8c4eac8837b4657aa39d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c1cad0c47b49c7af32d6734bcf5fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b791e7074d8d49d889f91d9aaf2af218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a9655a3b7a4d40b3eb24d03edab7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3bd15022b4486eab96324584ee0aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124e8d957fb24cbdae7e2daaa50625e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1725c4464046a5996f538925fc2a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b13876b723492fa1d55d3130c30941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c5082a1b694277bf109d7e47f6d968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:53:45,078 - INFO - Detected language: de with score 0.18\n",
      "2025-03-06 00:53:45,079 - INFO - Detected language: de\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd9a0b03fd14fc49bba62a0544f8fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e5d07efd054fd89dba3ce4fa1787f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7aa86894d4e4b67bffd39db46ffd63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab1849e431b4e10b12a91c411b096d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa69f21382d419db96f1066ca912be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbbb4c29c374b27bdf092aeeda50de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d623d2c39a46999b350a1b1ffa38b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888c1598b644453c969d2efffea04676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bf6bec30e345f4a99753d54d5acb67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ee3283245d4375b436a94038d00af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e928dcb3e80e4c7da98aa438a82577b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7163a54bad44738e71dadf5442698f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3a7c63930243408941797cee347971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:53:45,670 - INFO - Detected language: de with score 0.18\n",
      "2025-03-06 00:53:45,670 - INFO - Translation requested from de to en\n",
      "2025-03-06 00:53:45,671 - INFO - Translated query from de to English: Natrlich! Ich kann Ihnen helfen, sich auf ein Interview auf Deutsch vorzubereiten. Handelt es sich um ein Vorstellungsgesprch, ein akademisches Interview oder etwas anderes? Haben Sie spezifische Fragen oder Themen, auf die Sie sich konzentrieren mchten?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfe4efe82d9468d97da1c75be242b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174d3b26b8d9420691f42ef8134847bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ae4b3e566e4fc286a2267a717111cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801a5fd07e5f48a2aa7d15d56c561fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31aba937b8744f683c21865c42180dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57451db3c5ee451c916134c3dfa280ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0ba2d3921c49de9c3f6e85263a8cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697f11b7d65b4464991228bb45e2fb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2267937ea59f4612a198f7bf4a0ce179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fac158129b4da887a945351e9520c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d14d5cb4984efe9b6790456a8c110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb25d22eeae14d76af3fd71b11bda90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7508ad98274972a3512f086c8891e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 00:53:51,105 - INFO - Detected language: de with score 0.30\n",
      "2025-03-06 00:53:51,106 - INFO - Translated response to de\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gradio as gr\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import uuid\n",
    "import base64\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "import threading\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except:\n",
    "    logger.warning(\"NLTK download failed. Make sure NLTK is properly installed.\")\n",
    "\n",
    "class LoubbyDocumentation:\n",
    "    \"\"\"Class to handle Loubby platform documentation\"\"\"\n",
    "    \n",
    "    def __init__(self, docs_path=\"loubby_docs.txt\"):\n",
    "        \"\"\"Initialize the documentation handler\"\"\"\n",
    "        self.docs_path = docs_path\n",
    "        self.sections = {}\n",
    "        self.raw_text = self._load_documentation()\n",
    "        self.process_documentation()\n",
    "        \n",
    "    def _load_documentation(self) -> str:\n",
    "        \"\"\"Load documentation from file or create it if it doesn't exist\"\"\"\n",
    "        if not os.path.exists(self.docs_path):\n",
    "            logger.info(f\"Documentation file not found at {self.docs_path}. Creating it...\")\n",
    "            self._create_documentation_file()\n",
    "        \n",
    "        with open(self.docs_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    def _create_documentation_file(self):\n",
    "        \"\"\"Create the documentation file with Loubby platform information\"\"\"\n",
    "        documentation_text = \"\"\"# Loubby Platform Navigation Guide\n",
    "\n",
    "## Recruitment Portal\n",
    "\n",
    "### 1. Profile Setup\n",
    "#### 1.1 Creating Your Account\n",
    "To create your account on Loubby's recruitment portal:\n",
    "1. Navigate to loubby.ai/recruitment\n",
    "2. Click the \"Sign Up\" button in the top right corner\n",
    "3. Enter your email address and create a password\n",
    "4. Verify your email through the link sent to your inbox\n",
    "5. Complete your basic profile information (name, contact details)\n",
    "\n",
    "#### 1.2 Resume Upload\n",
    "To upload your resume to your Loubby profile:\n",
    "1. Log in to your account\n",
    "2. Navigate to \"My Profile\" from the dashboard menu\n",
    "3. Scroll to the \"Resume\" section\n",
    "4. Click \"Upload Resume\" button\n",
    "5. Select your resume file (supported formats: PDF, DOCX, RTF)\n",
    "6. Click \"Save Changes\" to confirm upload\n",
    "7. Verify that your resume appears in the preview section\n",
    "\n",
    "Tip: For best results, use a PDF format that is ATS-friendly\n",
    "\n",
    "#### 1.3 Skills Assessment\n",
    "To complete the skills assessment:\n",
    "1. From the dashboard, select \"Skills Assessment\" tab\n",
    "2. Choose the relevant skill categories for your profile\n",
    "3. For each category, click \"Start Assessment\"\n",
    "4. Complete the assessment questions within the allotted time\n",
    "5. Submit your answers using the \"Submit\" button\n",
    "6. View your results and skill ratings on completion\n",
    "\n",
    "### 2. Job Search and Application\n",
    "\n",
    "#### 2.1 Searching for Positions\n",
    "To search for open positions on Loubby:\n",
    "1. Click the \"Jobs\" tab in the main navigation\n",
    "2. Use the search bar to enter keywords, job titles, or company names\n",
    "3. Filter results using the sidebar options:\n",
    "   - Location (remote, on-site, hybrid)\n",
    "   - Experience level\n",
    "   - Industry\n",
    "   - Salary range\n",
    "   - Posted date\n",
    "4. Sort results by relevance, date, or salary using the dropdown menu\n",
    "5. Save searches by clicking \"Save This Search\" for future reference\n",
    "\n",
    "#### 2.2 Applying for a Position\n",
    "To apply for a position:\n",
    "1. From the job listing, click \"Apply Now\"\n",
    "2. Review your profile information for completeness\n",
    "3. Complete any additional application questions\n",
    "4. Upload any position-specific documents if requested\n",
    "5. Review all information for accuracy\n",
    "6. Click \"Submit Application\"\n",
    "7. Confirm submission in the popup window\n",
    "\n",
    "#### 2.3 Tracking Applications\n",
    "To track your submitted applications:\n",
    "1. Navigate to \"My Applications\" in the dashboard\n",
    "2. View all applications and their current status\n",
    "3. Filter by status (submitted, in review, interview scheduled, etc.)\n",
    "4. Click on any application to view details\n",
    "5. Check for any action items or requested information\n",
    "\n",
    "### 3. Interview Process\n",
    "\n",
    "#### 3.1 Scheduling Interviews\n",
    "When you receive an interview request:\n",
    "1. You'll receive an email notification and an in-platform notification\n",
    "2. Navigate to \"My Applications\" in the dashboard\n",
    "3. Find the application with the interview request\n",
    "4. Click on \"Schedule Interview\"\n",
    "5. Select from available time slots in the calendar interface\n",
    "6. Confirm your selection\n",
    "7. Add to your personal calendar using the calendar integration options\n",
    "\n",
    "#### 3.2 Virtual Interview Room\n",
    "To access the virtual interview room:\n",
    "1. 15 minutes before your scheduled interview, log in to Loubby\n",
    "2. Go to \"My Applications\" in the dashboard\n",
    "3. Find the relevant application\n",
    "4. Click \"Join Interview\" (button becomes active 15 minutes before start time)\n",
    "5. Test your audio and video in the pre-meeting room\n",
    "6. Click \"Join Now\" when ready\n",
    "7. If you experience technical difficulties, use the support chat in the bottom right\n",
    "\n",
    "## Employee Portal\n",
    "\n",
    "### 1. Onboarding\n",
    "\n",
    "#### 1.1 Accessing the Employee Portal\n",
    "After accepting a job offer:\n",
    "1. You'll receive an email with login credentials for the employee portal\n",
    "2. Navigate to loubby.ai/employee\n",
    "3. Enter your credentials from the email\n",
    "4. Set up two-factor authentication if prompted\n",
    "5. Create a new password that meets security requirements\n",
    "\n",
    "#### 1.2 Completing Onboarding Documents\n",
    "To complete necessary onboarding paperwork:\n",
    "1. From the employee dashboard, select \"Onboarding\" tab\n",
    "2. View the list of required documents\n",
    "3. Click \"Complete\" next to each document\n",
    "4. Fill out all required fields\n",
    "5. Use the e-signature tool where required\n",
    "6. Click \"Submit\" for each completed document\n",
    "7. Track completion progress in the onboarding checklist\n",
    "\n",
    "#### 1.3 Training Modules\n",
    "To access and complete training modules:\n",
    "1. Navigate to \"Training\" in the main menu\n",
    "2. View assigned training modules and their deadlines\n",
    "3. Click on a module to begin\n",
    "4. Complete all sections of the module\n",
    "5. Take the assessment quiz at the end\n",
    "6. Achieve the minimum required score to mark as complete\n",
    "7. View your training transcript in the \"My Learning\" section\n",
    "\n",
    "### 2. Course Synchronization\n",
    "\n",
    "#### 2.1 Connecting External Courses\n",
    "To sync external courses with the Loubby platform:\n",
    "1. From the dashboard, navigate to \"Education & Courses\"\n",
    "2. Select \"Connect External Course\" button\n",
    "3. Choose from the list of supported learning platforms\n",
    "4. Log in to your account on the selected platform\n",
    "5. Authorize the connection when prompted\n",
    "6. Select which courses to sync with Loubby\n",
    "7. Click \"Confirm\" to complete the integration\n",
    "\n",
    "#### 2.2 Viewing Course Progress\n",
    "To monitor your progress across all courses:\n",
    "1. Go to \"My Learning\" in the main navigation\n",
    "2. View the dashboard showing all courses and completion percentages\n",
    "3. Filter by course status, start date, or provider\n",
    "4. Click on any course to view detailed progress\n",
    "5. See upcoming assignments and deadlines\n",
    "6. View instructor feedback on completed assignments\n",
    "\n",
    "#### 2.3 Submitting Assignments\n",
    "To submit assignments through Loubby:\n",
    "1. Navigate to \"My Learning\" > \"Assignments\"\n",
    "2. Find the assignment you want to submit\n",
    "3. Click \"Submit Assignment\"\n",
    "4. Upload required files or enter text submission\n",
    "5. Add any comments for the instructor\n",
    "6. Review your submission\n",
    "7. Click \"Submit\" to finalize\n",
    "\n",
    "### 3. Performance Tracking\n",
    "\n",
    "#### 3.1 Setting Goals\n",
    "To set and track professional goals:\n",
    "1. From the employee dashboard, select \"Performance\" tab\n",
    "2. Click \"My Goals\" in the submenu\n",
    "3. Select \"Add New Goal\"\n",
    "4. Define your goal with the SMART criteria\n",
    "5. Set target date and key milestones\n",
    "6. Link to relevant skills or competencies\n",
    "7. Submit for manager approval\n",
    "\n",
    "#### 3.2 Feedback and Reviews\n",
    "To access performance feedback:\n",
    "1. Navigate to \"Performance\" > \"Feedback\"\n",
    "2. View all feedback received from peers and supervisors\n",
    "3. Filter by date, project, or feedback type\n",
    "4. For formal reviews, select \"Reviews\" from the submenu\n",
    "5. View scheduled and past review cycles\n",
    "6. Complete self-assessment when prompted\n",
    "7. Review manager feedback and ratings\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "#### Password Reset\n",
    "If you need to reset your password:\n",
    "1. On the login page, click \"Forgot Password\"\n",
    "2. Enter the email associated your account\n",
    "3. Check your email for a reset link\n",
    "4. Click the link and follow instructions to create a new password\n",
    "5. If you don't receive the email, check spam folder or click \"Resend Email\"\n",
    "\n",
    "#### Browser Compatibility\n",
    "Loubby works best with:\n",
    "- Chrome (version 90+)\n",
    "- Firefox (version 88+)\n",
    "- Safari (version 14+)\n",
    "- Edge (version 90+)\n",
    "\n",
    "If experiencing issues:\n",
    "1. Update your browser to the latest version\n",
    "2. Clear browser cache and cookies\n",
    "3. Disable extensions that might interfere\n",
    "4. Try an alternate supported browser\n",
    "\n",
    "#### Mobile Access\n",
    "To access Loubby on mobile devices:\n",
    "1. Use the responsive web version on any mobile browser\n",
    "2. Download the Loubby mobile app from App Store or Google Play\n",
    "3. Log in with the same credentials as the web version\n",
    "4. Enable notifications for interview alerts\n",
    "5. Note that some advanced features may require desktop access\n",
    "\n",
    "### Getting Help\n",
    "\n",
    "#### Live Support\n",
    "To access live support:\n",
    "1. Click the \"Help\" icon in the bottom right corner\n",
    "2. Select \"Chat with Support\"\n",
    "3. Describe your issue in detail\n",
    "4. Support is available Monday-Friday, 9am-6pm EST\n",
    "\n",
    "#### Knowledge Base\n",
    "To search the help documentation:\n",
    "1. Click \"Help\" in the main navigation\n",
    "2. Use the search bar to find relevant articles\n",
    "3. Browse by category using the sidebar\n",
    "4. Rate articles to help improve documentation\n",
    "\"\"\"\n",
    "        with open(self.docs_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(documentation_text)\n",
    "        \n",
    "        logger.info(f\"Documentation file created at {self.docs_path}\")\n",
    "    \n",
    "    def process_documentation(self):\n",
    "        \"\"\"Process the documentation into sections\"\"\"\n",
    "        # Split by main sections using regex\n",
    "        section_pattern = r'(#+)\\s+(.+)'\n",
    "        current_section = {\"title\": \"Root\", \"content\": \"\", \"level\": 0, \"subsections\": []}\n",
    "        section_stack = [current_section]\n",
    "        \n",
    "        for line in self.raw_text.split('\\n'):\n",
    "            match = re.match(section_pattern, line)\n",
    "            if match:\n",
    "                level = len(match.group(1))\n",
    "                title = match.group(2).strip()\n",
    "                \n",
    "                new_section = {\"title\": title, \"content\": \"\", \"level\": level, \"subsections\": []}\n",
    "                \n",
    "                # Pop from stack until we find the parent section\n",
    "                while level <= section_stack[-1][\"level\"]:\n",
    "                    section_stack.pop()\n",
    "                \n",
    "                # Add new section as subsection of parent\n",
    "                section_stack[-1][\"subsections\"].append(new_section)\n",
    "                \n",
    "                # Push new section to stack\n",
    "                section_stack.append(new_section)\n",
    "            else:\n",
    "                # Add content to current section\n",
    "                if section_stack[-1][\"content\"]:\n",
    "                    section_stack[-1][\"content\"] += \"\\n\" + line\n",
    "                else:\n",
    "                    section_stack[-1][\"content\"] += line\n",
    "        \n",
    "        self.sections = section_stack[0]\n",
    "        logger.info(\"Documentation processed successfully\")\n",
    "\n",
    "    def get_flattened_sections(self) -> List[Dict]:\n",
    "        \"\"\"Flatten the hierarchical sections into a list with full paths\"\"\"\n",
    "        flattened = []\n",
    "        \n",
    "        def traverse(section, path=\"\"):\n",
    "            current_path = f\"{path} > {section['title']}\" if path else section['title']\n",
    "            \n",
    "            # Add current section\n",
    "            if section['content'].strip():\n",
    "                flattened.append({\n",
    "                    \"path\": current_path,\n",
    "                    \"title\": section['title'],\n",
    "                    \"content\": section['content'],\n",
    "                    \"level\": section['level']\n",
    "                })\n",
    "            \n",
    "            # Process subsections\n",
    "            for subsection in section['subsections']:\n",
    "                traverse(subsection, current_path)\n",
    "        \n",
    "        traverse(self.sections)\n",
    "        return flattened\n",
    "\n",
    "class DocumentChunker:\n",
    "    \"\"\"Class to chunk documentation for efficient retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=200, chunk_overlap=50):\n",
    "        \"\"\"Initialize the document chunker\"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "    \n",
    "    def chunk_document(self, sections: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Split sections into smaller chunks for retrieval\"\"\"\n",
    "        chunked_sections = []\n",
    "        \n",
    "        for section in sections:\n",
    "            # Combine section info with content for context\n",
    "            full_text = f\"{section['path']}\\n\\n{section['content']}\"\n",
    "            \n",
    "            # Split text into chunks\n",
    "            chunks = self.text_splitter.split_text(full_text)\n",
    "            \n",
    "            # Create dictionary entries for each chunk\n",
    "            for i, chunk_text in enumerate(chunks):\n",
    "                chunked_sections.append({\n",
    "                    \"id\": f\"{section['path']}_{i}\",\n",
    "                    \"path\": section['path'],\n",
    "                    \"title\": section['title'],\n",
    "                    \"chunk_index\": i,\n",
    "                    \"content\": chunk_text,\n",
    "                    \"level\": section['level']\n",
    "                })\n",
    "        \n",
    "        logger.info(f\"Created {len(chunked_sections)} chunks from {len(sections)} sections\")\n",
    "        return chunked_sections\n",
    "\n",
    "class VectorDatabase:\n",
    "    \"\"\"Class to manage vector embeddings and similarity search\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the vector database\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        self.vectorstore = None\n",
    "        self.chunk_data = None\n",
    "    \n",
    "    def create_vectors(self, chunked_sections: List[Dict]):\n",
    "        \"\"\"Create vector embeddings for document chunks\"\"\"\n",
    "        # Extract texts and metadata\n",
    "        texts = [chunk[\"content\"] for chunk in chunked_sections]\n",
    "        metadatas = [{\n",
    "            \"id\": chunk[\"id\"],\n",
    "            \"path\": chunk[\"path\"],\n",
    "            \"title\": chunk[\"title\"],\n",
    "            \"chunk_index\": chunk[\"chunk_index\"],\n",
    "            \"level\": chunk[\"level\"]\n",
    "        } for chunk in chunked_sections]\n",
    "        \n",
    "        # Create FAISS index\n",
    "        self.vectorstore = FAISS.from_texts(\n",
    "            texts=texts,\n",
    "            embedding=self.embeddings,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "        \n",
    "        # Store the original chunk data for reference\n",
    "        self.chunk_data = {chunk[\"id\"]: chunk for chunk in chunked_sections}\n",
    "        \n",
    "        logger.info(f\"Created vector database with {len(texts)} documents\")\n",
    "        return self.vectorstore\n",
    "    \n",
    "    def save_vectors(self, path=\"loubby_vectors\"):\n",
    "        \"\"\"Save vector database to disk\"\"\"\n",
    "        if self.vectorstore:\n",
    "            self.vectorstore.save_local(path)\n",
    "            # Save chunk data separately\n",
    "            with open(f\"{path}_chunks.json\", 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.chunk_data, f)\n",
    "            logger.info(f\"Vector database saved to {path}\")\n",
    "        else:\n",
    "            logger.error(\"No vector database to save\")\n",
    "    \n",
    "    def load_vectors(self, path=\"loubby_vectors\"):\n",
    "        \"\"\"Load vector database from disk\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                self.vectorstore = FAISS.load_local(path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "                # Load chunk data\n",
    "                with open(f\"{path}_chunks.json\", 'r', encoding='utf-8') as f:\n",
    "                    self.chunk_data = json.load(f)\n",
    "                logger.info(f\"Vector database loaded from {path}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading vector database: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            logger.warning(f\"No vector database found at {path}\")\n",
    "            return False\n",
    "    \n",
    "    def similarity_search(self, query: str, k=3) -> List[Dict]:\n",
    "        \"\"\"Search for most similar chunks to the query\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            logger.error(\"Vector database not initialized\")\n",
    "            return []\n",
    "        \n",
    "        results = self.vectorstore.similarity_search_with_score(query, k=k)\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        for doc, score in results:\n",
    "            chunk_id = doc.metadata[\"id\"]\n",
    "            formatted_results.append({\n",
    "                \"id\": chunk_id,\n",
    "                \"path\": doc.metadata[\"path\"],\n",
    "                \"title\": doc.metadata[\"title\"],\n",
    "                \"content\": doc.page_content,\n",
    "                \"similarity\": float(score),\n",
    "                \"chunk_index\": doc.metadata[\"chunk_index\"],\n",
    "                \"level\": doc.metadata[\"level\"]\n",
    "            })\n",
    "        \n",
    "        return formatted_results\n",
    "\n",
    "class FeedbackSystem:\n",
    "    \"\"\"System to collect and use user feedback to improve responses\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path=\"loubby_feedback.db\"):\n",
    "        \"\"\"Initialize the feedback system\"\"\"\n",
    "        self.db_path = db_path\n",
    "        self._init_database()\n",
    "    \n",
    "    def _init_database(self):\n",
    "        \"\"\"Initialize the SQLite database for feedback\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Create feedback table if it doesn't exist\n",
    "            cursor.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS feedback (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    query TEXT NOT NULL,\n",
    "                    response TEXT NOT NULL,\n",
    "                    rating INTEGER NOT NULL,\n",
    "                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "                    user_comment TEXT\n",
    "                )\n",
    "            ''')\n",
    "            \n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            logger.info(f\"Feedback database initialized at {self.db_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing feedback database: {e}\")\n",
    "    \n",
    "    def record_feedback(self, query: str, response: str, rating: int, user_comment: str = \"\"):\n",
    "        \"\"\"Record user feedback on a response\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\n",
    "                \"INSERT INTO feedback (query, response, rating, user_comment) VALUES (?, ?, ?, ?)\",\n",
    "                (query, response, rating, user_comment)\n",
    "            )\n",
    "            \n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            logger.info(f\"Feedback recorded: Rating {rating} for query '{query[:30]}...'\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recording feedback: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_historical_feedback(self, limit=100) -> List[Dict]:\n",
    "        \"\"\"Get historical feedback data\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            conn.row_factory = sqlite3.Row\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(\n",
    "                \"SELECT * FROM feedback ORDER BY timestamp DESC LIMIT ?\",\n",
    "                (limit,)\n",
    "            )\n",
    "            \n",
    "            # Convert to list of dictionaries\n",
    "            results = [dict(row) for row in cursor.fetchall()]\n",
    "            \n",
    "            conn.close()\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching historical feedback: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_feedback_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about feedback ratings\"\"\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Get average rating\n",
    "            cursor.execute(\"SELECT AVG(rating) FROM feedback\")\n",
    "            avg_rating = cursor.fetchone()[0] or 0\n",
    "            \n",
    "            # Get rating distribution\n",
    "            cursor.execute(\n",
    "                \"SELECT rating, COUNT(*) FROM feedback GROUP BY rating ORDER BY rating\"\n",
    "            )\n",
    "            rating_counts = dict(cursor.fetchall())\n",
    "            \n",
    "            # Get total feedback count\n",
    "            cursor.execute(\"SELECT COUNT(*) FROM feedback\")\n",
    "            total_count = cursor.fetchone()[0]\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            return {\n",
    "                \"average_rating\": round(avg_rating, 2),\n",
    "                \"rating_distribution\": rating_counts,\n",
    "                \"total_count\": total_count\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching feedback statistics: {e}\")\n",
    "            return {\n",
    "                \"average_rating\": 0,\n",
    "                \"rating_distribution\": {},\n",
    "                \"total_count\": 0\n",
    "            }\n",
    "\n",
    "class LanguageProcessor:\n",
    "    \"\"\"Class for handling natural language processing tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=\"google/flan-t5-base\"):\n",
    "        \"\"\"Initialize the language processor\"\"\"\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "            logger.info(f\"Language processor initialized with model {model_path}\")\n",
    "            \n",
    "            # Load language detection model\n",
    "            self.lang_detector = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "            self.language_map = {\n",
    "                \"en\": \"English\",\n",
    "                \"es\": \"Spanish\", \n",
    "                \"fr\": \"French\",\n",
    "                \"de\": \"German\",\n",
    "                \"zh\": \"Chinese\",\n",
    "                \"ja\": \"Japanese\",\n",
    "                \"ru\": \"Russian\",\n",
    "                \"ar\": \"Arabic\",\n",
    "                \"hi\": \"Hindi\",\n",
    "                \"pt\": \"Portuguese\",\n",
    "                \"it\": \"Italian\",\n",
    "                \"ko\": \"Korean\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing language processor: {e}\")\n",
    "            self.tokenizer = None\n",
    "            self.model = None\n",
    "    \n",
    "    def detect_language(self, text: str) -> str:\n",
    "        \"\"\"Detect the language of input text using a multilingual sentence transformer\"\"\"\n",
    "        if not text.strip():\n",
    "            return \"en\"  # Default to English for empty text\n",
    "        \n",
    "        try:\n",
    "            # Get embeddings for the text\n",
    "            text_embedding = self.lang_detector.encode(text)\n",
    "            \n",
    "            # Compare with embeddings of language markers\n",
    "            language_scores = {}\n",
    "            for lang, lang_name in self.language_map.items():\n",
    "                # Create a simple sentence in the target language\n",
    "                lang_marker = f\"This is a sentence in {lang_name}.\"\n",
    "                lang_embedding = self.lang_detector.encode(lang_marker)\n",
    "                \n",
    "                # Calculate cosine similarity\n",
    "                similarity = cosine_similarity([text_embedding], [lang_embedding])[0][0]\n",
    "                language_scores[lang] = similarity\n",
    "            \n",
    "            # Return the language with the highest similarity\n",
    "            detected_lang = max(language_scores.items(), key=lambda x: x[1])[0]\n",
    "            logger.info(f\"Detected language: {detected_lang} with score {language_scores[detected_lang]:.2f}\")\n",
    "            \n",
    "            return detected_lang\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error detecting language: {e}\")\n",
    "            return \"en\"  # Default to English on error\n",
    "    \n",
    "    def translate_text(self, text: str, target_language: str = \"en\") -> str:\n",
    "        \"\"\"Simple translation function (placeholder for actual translation)\"\"\"\n",
    "        # Note: In a real implementation, you would integrate with a translation API\n",
    "        # Since we're omitting external API dependencies for this cloud environment, \n",
    "        # we'll return the original text with a note\n",
    "        \n",
    "        detected_language = self.detect_language(text)\n",
    "        \n",
    "        # Skip translation if already in target language\n",
    "        if detected_language == target_language:\n",
    "            return text\n",
    "        \n",
    "        logger.info(f\"Translation requested from {detected_language} to {target_language}\")\n",
    "        \n",
    "        # In a real implementation, you would call a translation service here\n",
    "        # For now, just return the original text\n",
    "        return text\n",
    "    \n",
    "    def generate_response(self, query: str, retrieved_chunks: List[Dict]) -> str:\n",
    "        \"\"\"Generate a comprehensive response based on the retrieved chunks\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            return \"Language model not properly initialized.\"\n",
    "        \n",
    "        try:\n",
    "            # Create a context from retrieved chunks\n",
    "            context = \"\\n\\n\".join([chunk[\"content\"] for chunk in retrieved_chunks])\n",
    "            \n",
    "            # Build a prompt\n",
    "            prompt = f\"\"\"\n",
    "            CONTEXT: {context}\n",
    "            \n",
    "            QUERY: {query}\n",
    "            \n",
    "            Based on the above context, provide a clear and concise answer to the query about navigating the Loubby platform.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Generate response\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "            outputs = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=512,\n",
    "                min_length=64,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response: {e}\")\n",
    "            \n",
    "            # Fallback response method\n",
    "            most_relevant_chunk = retrieved_chunks[0] if retrieved_chunks else None\n",
    "            if most_relevant_chunk:\n",
    "                return f\"Based on the Loubby documentation, here's guidance on your question:\\n\\n{most_relevant_chunk['content']}\"\n",
    "            else:\n",
    "                return \"I'm unable to generate a specific response at the moment. Please try rephrasing your question about the Loubby platform.\"\n",
    "\n",
    "class VoiceInterface:\n",
    "    \"\"\"Class to handle voice input and output using Gradio components\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the voice interface\"\"\"\n",
    "        logger.info(\"Voice interface initialized using Gradio components\")\n",
    "    \n",
    "    def process_audio(self, audio):\n",
    "        \"\"\"Process audio input and return text\"\"\"\n",
    "        try:\n",
    "            if audio is None:\n",
    "                return \"\"\n",
    "            \n",
    "            # Use Gradio's built-in audio processing\n",
    "            recognizer = sr.Recognizer()\n",
    "            with sr.AudioFile(audio) as source:\n",
    "                audio_data = recognizer.record(source)\n",
    "                text = recognizer.recognize_google(audio_data)\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing audio: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def generate_audio(self, text, language=\"en\"):\n",
    "        \"\"\"Generate audio from text with language support\"\"\"\n",
    "        try:\n",
    "            if not text.strip():\n",
    "                return None\n",
    "            \n",
    "            tts = gTTS(text=text, lang=language)\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False) as f:\n",
    "                tts.save(f.name)\n",
    "                return f.name\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating audio: {e}\")\n",
    "            return None\n",
    "\n",
    "class VideoInterface:\n",
    "    \"\"\"Class to handle video communication in cloud environment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the video interface with dummy video for cloud environments\"\"\"\n",
    "        self.is_running = False\n",
    "        self.thread = None\n",
    "        self.frame = None\n",
    "        self.user_face_detected = False\n",
    "        self.animation_frame = 0\n",
    "        self.animation_speed = 0.15\n",
    "        self.last_animation_time = time.time()\n",
    "        \n",
    "        # Create avatars since we can't use real camera\n",
    "        self.user_avatar = self._create_user_avatar()\n",
    "        self.assistant_avatar = self._create_assistant_avatar()\n",
    "        \n",
    "        logger.info(\"Video interface initialized with virtual avatars for cloud environment\")\n",
    "    \n",
    "    def _create_user_avatar(self):\n",
    "        \"\"\"Create a simple avatar for the user in cloud environments\"\"\"\n",
    "        avatar = np.zeros((240, 320, 3), dtype=np.uint8)\n",
    "        # Set background color\n",
    "        avatar[:, :] = (50, 50, 50)\n",
    "        # Draw a simple circle face\n",
    "        cv2.circle(avatar, (160, 120), 80, (100, 100, 220), -1)\n",
    "        # Draw eyes\n",
    "        cv2.circle(avatar, (130, 100), 15, (255, 255, 255), -1)\n",
    "        cv2.circle(avatar, (190, 100), 15, (255, 255, 255), -1)\n",
    "        cv2.circle(avatar, (130, 100), 7, (0, 0, 0), -1)\n",
    "        cv2.circle(avatar, (190, 100), 7, (0, 0, 0), -1)\n",
    "        # Draw mouth\n",
    "        cv2.ellipse(avatar, (160, 140), (40, 20), 0, 0, 180, (0, 0, 0), 3)\n",
    "        \n",
    "        # Add text label\n",
    "        cv2.putText(avatar, \"User (virtual)\", (80, 210), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "        cv2.putText(avatar, \"Cloud Environment\", (70, 230), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        \n",
    "        return avatar\n",
    "    \n",
    "    def _create_assistant_avatar(self):\n",
    "        \"\"\"Create a simple avatar for the assistant\"\"\"\n",
    "        avatar = np.zeros((240, 320, 3), dtype=np.uint8)\n",
    "        # Set background color\n",
    "        avatar[:, :] = (30, 30, 30)\n",
    "        # Draw a simple circle face\n",
    "        cv2.circle(avatar, (160, 120), 80, (70, 130, 180), -1)\n",
    "        # Draw eyes\n",
    "        cv2.circle(avatar, (130, 100), 15, (255, 255, 255), -1)\n",
    "        cv2.circle(avatar, (190, 100), 15, (255, 255, 255), -1)\n",
    "        cv2.circle(avatar, (130, 100), 7, (0, 0, 0), -1)\n",
    "        cv2.circle(avatar, (190, 100), 7, (0, 0, 0), -1)\n",
    "        # Draw mouth (will be animated)\n",
    "        cv2.ellipse(avatar, (160, 140), (40, 15), 0, 0, 180, (0, 0, 0), 3)\n",
    "        \n",
    "        # Add text label\n",
    "        cv2.putText(avatar, \"Loubby Assistant\", (100, 210), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "        \n",
    "        return avatar\n",
    "    \n",
    "    def _update_avatar_animation(self):\n",
    "        \"\"\"Update the avatar's animation\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_animation_time > self.animation_speed:\n",
    "            # Create a copy of the base avatar\n",
    "            animated_avatar = self.assistant_avatar.copy()\n",
    "            \n",
    "            # Animate the mouth based on speaking state\n",
    "            mouth_height = 10 + 10 * np.sin(self.animation_frame)\n",
    "            cv2.ellipse(animated_avatar, (160, 140), (40, int(mouth_height)), 0, 0, 180, (0, 0, 0), 3)\n",
    "            \n",
    "            self.animation_frame += 0.5\n",
    "            self.last_animation_time = current_time\n",
    "            \n",
    "            return animated_avatar\n",
    "        return self.assistant_avatar.copy()\n",
    "    \n",
    "    def start_video(self):\n",
    "        \"\"\"Start the virtual video session\"\"\"\n",
    "        if self.is_running:\n",
    "            return True\n",
    "        \n",
    "        self.is_running = True\n",
    "        \n",
    "        # Start animation thread\n",
    "        self.thread = threading.Thread(target=self._animate_avatars)\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "        \n",
    "        logger.info(\"Virtual video avatars started for cloud environment\")\n",
    "        return True\n",
    "    \n",
    "    def stop_video(self):\n",
    "        \"\"\"Stop the virtual video session\"\"\"\n",
    "        self.is_running = False\n",
    "        \n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=1.0)\n",
    "            self.thread = None\n",
    "        \n",
    "        logger.info(\"Virtual video avatars stopped\")\n",
    "        return True\n",
    "    \n",
    "    def _animate_avatars(self):\n",
    "        \"\"\"Animate the avatars in a continuous loop\"\"\"\n",
    "        while self.is_running:\n",
    "            # Update the assistant avatar animation\n",
    "            self._update_avatar_animation()\n",
    "            time.sleep(0.05)\n",
    "    \n",
    "    def get_user_frame(self):\n",
    "        \"\"\"Get the user's avatar frame\"\"\"\n",
    "        return self.user_avatar\n",
    "    \n",
    "    def get_assistant_frame(self):\n",
    "        \"\"\"Get the assistant's animated avatar frame\"\"\"\n",
    "        return self._update_avatar_animation()\n",
    "    \n",
    "    def generate_combined_frame(self):\n",
    "        \"\"\"Generate a combined frame with user and assistant avatars\"\"\"\n",
    "        user_frame = self.get_user_frame()\n",
    "        assistant_frame = self.get_assistant_frame()\n",
    "        \n",
    "        # Create a combined frame (side by side)\n",
    "        combined = np.zeros((240, 640, 3), dtype=np.uint8)\n",
    "        combined[:, :320] = user_frame\n",
    "        combined[:, 320:] = assistant_frame\n",
    "        \n",
    "        # Add a separating line\n",
    "        cv2.line(combined, (320, 0), (320, 240), (100, 100, 100), 2)\n",
    "        \n",
    "        # Add a status indicator showing this is a cloud environment\n",
    "        cv2.rectangle(combined, (180, 10), (460, 30), (0, 0, 100), -1)\n",
    "        cv2.putText(combined, \"Cloud Environment - Virtual Video\", (190, 25), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def process_video_call(self, text_message):\n",
    "        \"\"\"Process a simulated video call with the assistant\"\"\"\n",
    "        if not self.is_running:\n",
    "            return \"Please start the video call first.\", self._create_assistant_avatar()\n",
    "        \n",
    "        # Generate a combined frame with user and assistant\n",
    "        combined_frame = self.generate_combined_frame()\n",
    "        \n",
    "        if not text_message:\n",
    "            return \"I'm here and ready to help! Type a message to ask about the Loubby platform.\", combined_frame\n",
    "        \n",
    "        return f\"Processing: '{text_message}'\", combined_frame\n",
    "\n",
    "class LoubbyAssistant:\n",
    "    \"\"\"Main assistant class that integrates all components\"\"\"\n",
    "    \n",
    "    def __init__(self, load_existing=True):\n",
    "        \"\"\"Initialize the Loubby Assistant\"\"\"\n",
    "        self.documentation = LoubbyDocumentation()\n",
    "        self.chunker = DocumentChunker(chunk_size=300, chunk_overlap=100)\n",
    "        self.vector_db = VectorDatabase()\n",
    "        self.feedback_system = FeedbackSystem()\n",
    "        self.language_processor = LanguageProcessor()\n",
    "        self.voice_interface = VoiceInterface()\n",
    "        self.video_interface = VideoInterface()\n",
    "        \n",
    "        # Initialize vector database\n",
    "        if load_existing and self.vector_db.load_vectors():\n",
    "            logger.info(\"Using existing vector database\")\n",
    "        else:\n",
    "            logger.info(\"Creating new vector database\")\n",
    "            sections = self.documentation.get_flattened_sections()\n",
    "            chunks = self.chunker.chunk_document(sections)\n",
    "            self.vector_db.create_vectors(chunks)\n",
    "            self.vector_db.save_vectors()\n",
    "        \n",
    "        # Track conversation history\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # Map of language codes to full names for internal use\n",
    "        self.language_map = {\n",
    "            \"en\": \"English\",\n",
    "            \"es\": \"Spanish\", \n",
    "            \"fr\": \"French\",\n",
    "            \"de\": \"German\",\n",
    "            \"zh\": \"Chinese\",\n",
    "            \"ja\": \"Japanese\",\n",
    "            \"ru\": \"Russian\",\n",
    "            \"ar\": \"Arabic\",\n",
    "            \"hi\": \"Hindi\",\n",
    "            \"pt\": \"Portuguese\",\n",
    "            \"it\": \"Italian\",\n",
    "            \"ko\": \"Korean\"\n",
    "        }\n",
    "        \n",
    "        logger.info(\"Loubby Assistant initialized and ready\")\n",
    "    \n",
    "    async def process_query(self, query: str, use_voice=False) -> Dict:\n",
    "        \"\"\"Process a user query and generate a response with language detection\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Detect language (using our improved detection method)\n",
    "        detected_language = self.language_processor.detect_language(query)\n",
    "        logger.info(f\"Detected language: {detected_language}\")\n",
    "        \n",
    "        translated_query = query\n",
    "        \n",
    "        # Translate to English for processing if not already in English\n",
    "        if detected_language != \"en\":\n",
    "            translated_query = self.language_processor.translate_text(query, target_language=\"en\")\n",
    "            logger.info(f\"Translated query from {detected_language} to English: {translated_query}\")\n",
    "        \n",
    "        # Search for relevant chunks\n",
    "        retrieved_chunks = self.vector_db.similarity_search(translated_query, k=3)\n",
    "        \n",
    "        # Generate response in English\n",
    "        response_en = \"\"\n",
    "        if retrieved_chunks:\n",
    "            response_en = self.language_processor.generate_response(translated_query, retrieved_chunks)\n",
    "        else:\n",
    "            response_en = \"I'm sorry, I couldn't find specific information about that in the Loubby documentation. Could you please rephrase your question about using the Loubby platform?\"\n",
    "        \n",
    "        # Translate response back to original language if needed\n",
    "        response = response_en\n",
    "        if detected_language != \"en\":\n",
    "            try:\n",
    "               response = self.language_processor.translate_text(response_en, target_language=detected_language)\n",
    "               logger.info(f\"Translated response to {detected_language}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error Translating response to {detected_language}: {e}\")\n",
    "                response=response_en\n",
    "                \n",
    "        # Add sources information\n",
    "        sources = []\n",
    "        for chunk in retrieved_chunks:\n",
    "            if chunk[\"path\"] not in [s[\"path\"] for s in sources]:\n",
    "                sources.append({\n",
    "                    \"path\": chunk[\"path\"],\n",
    "                    \"title\": chunk[\"title\"]\n",
    "                })\n",
    "        \n",
    "        # Use voice interface if requested\n",
    "        audio_file = None\n",
    "        if use_voice:\n",
    "            audio_file = self.voice_interface.generate_audio(response, language=detected_language)\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"sources\": sources,\n",
    "            \"language\": detected_language\n",
    "        })\n",
    "        \n",
    "        # Calculate processing time\n",
    "        processing_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"sources\": sources,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"detected_language\": detected_language,\n",
    "            \"audio_file\": audio_file,\n",
    "            \"language\": detected_language\n",
    "        }\n",
    "    \n",
    "    async def voice_query(self, audio):\n",
    "        \"\"\"Handle a voice query from the user\"\"\"\n",
    "        # Process audio input\n",
    "        query = self.voice_interface.process_audio(audio)\n",
    "        \n",
    "        if not query:\n",
    "            response = {\n",
    "                \"query\": \"\",\n",
    "                \"response\": \"I didn't hear anything. Please try speaking again.\",\n",
    "                \"sources\": [],\n",
    "                \"processing_time\": 0,\n",
    "                \"detected_language\": \"en\",\n",
    "                \"audio_file\": None,\n",
    "                \"language\": \"en\"\n",
    "            }\n",
    "            return response[\"query\"], response[\"response\"], \", \".join([s for s in []]), response[\"audio_file\"]\n",
    "        \n",
    "        # Process the query\n",
    "        result = await self.process_query(query, use_voice=True)\n",
    "        return result[\"query\"], result[\"response\"], \", \".join([s[\"path\"] for s in result[\"sources\"]]), result[\"audio_file\"]\n",
    "    \n",
    "    async def video_chat_query(self, message):\n",
    "        \"\"\"Handle a text query during video call\"\"\"\n",
    "        if not message.strip():\n",
    "            return \"Please type a message to continue the conversation.\"\n",
    "            \n",
    "        # Process the query as normal text query\n",
    "        result = await self.process_query(message)\n",
    "        return result[\"response\"]\n",
    "    \n",
    "    def record_feedback(self, query: str, response: str, rating: int, comment: str = \"\") -> bool:\n",
    "        \"\"\"Record user feedback on a response\"\"\"\n",
    "        return self.feedback_system.record_feedback(query, response, rating, comment)\n",
    "    \n",
    "    def get_feedback_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about feedback ratings\"\"\"\n",
    "        return self.feedback_system.get_feedback_statistics()\n",
    "    \n",
    "    def get_conversation_history(self) -> List[Dict]:\n",
    "        \"\"\"Get the conversation history\"\"\"\n",
    "        return self.conversation_history\n",
    "\n",
    "# Gradio Interface\n",
    "def create_interface(assistant):\n",
    "    \"\"\"Create a Gradio interface for the Loubby Assistant\"\"\"\n",
    "    \n",
    "    async def process_text_query(query, voice_output=False):\n",
    "        \"\"\"Process a text query and return the response with automatic language detection\"\"\"\n",
    "        if not query.strip():\n",
    "            return \"Please enter a question about the Loubby platform.\", \"\", None, \"No query detected\"\n",
    "            \n",
    "        result = await assistant.process_query(query, use_voice=voice_output)\n",
    "        detected_language = result[\"detected_language\"]\n",
    "        language_name = assistant.language_map.get(detected_language, detected_language)\n",
    "        \n",
    "        return result[\"response\"], \", \".join([s[\"path\"] for s in result[\"sources\"]]), result[\"audio_file\"], f\"Detected language: {language_name}\"\n",
    "    \n",
    "    async def process_voice_query(audio):\n",
    "        \"\"\"Process a voice query and return the response\"\"\"\n",
    "        query, response, sources, audio_file = await assistant.voice_query(audio)\n",
    "        \n",
    "        # Detect language for display\n",
    "        if query:\n",
    "            detected_language = assistant.language_processor.detect_language(query)\n",
    "            language_name = assistant.language_map.get(detected_language, detected_language)\n",
    "            language_info = f\"Detected language: {language_name}\"\n",
    "        else:\n",
    "            language_info = \"No speech detected\"\n",
    "            \n",
    "        return query, response, sources, audio_file, language_info\n",
    "    \n",
    "    async def submit_feedback(query, response, rating, comment):\n",
    "        \"\"\"Submit feedback for a response\"\"\"\n",
    "        success = assistant.record_feedback(query, response, rating, comment)\n",
    "        return \"Feedback submitted successfully. Thank you!\" if success else \"Failed to submit feedback. Please try again.\"\n",
    "    \n",
    "    async def show_statistics():\n",
    "        \"\"\"Show feedback statistics\"\"\"\n",
    "        stats = assistant.get_feedback_statistics()\n",
    "        return f\"\"\"\n",
    "        Feedback Statistics:\n",
    "        - Average Rating: {stats['average_rating']} / 5\n",
    "        - Total Feedback Count: {stats['total_count']}\n",
    "        - Rating Distribution: {stats['rating_distribution']}\n",
    "        \"\"\"\n",
    "    \n",
    "    # Video chat functionality with virtual avatars for cloud environment\n",
    "    def start_video_call():\n",
    "        \"\"\"Start video capture for video chat\"\"\"\n",
    "        success = assistant.video_interface.start_video()\n",
    "        status = \"Video call started with virtual avatars (cloud environment mode).\" if success else \"Failed to start video call.\"\n",
    "        return status\n",
    "    \n",
    "    def end_video_call():\n",
    "        \"\"\"End video capture for video chat\"\"\"\n",
    "        success = assistant.video_interface.stop_video()\n",
    "        status = \"Video call ended.\" if success else \"Error ending video call.\"\n",
    "        return status\n",
    "    \n",
    "    async def video_chat_message(message, chat_history):\n",
    "        \"\"\"Handle chat messages during video calls\"\"\"\n",
    "        if not message.strip():\n",
    "            processed_message = \"I'm here to help with Loubby platform navigation.\"\n",
    "        else:\n",
    "            processed_message = await assistant.video_chat_query(message)\n",
    "        \n",
    "        # Get combined video frame\n",
    "        response_text, video_frame = assistant.video_interface.process_video_call(message)\n",
    "        \n",
    "        # Update chat history\n",
    "        chat_history = chat_history or []\n",
    "        chat_history.append((message, processed_message))\n",
    "        \n",
    "        return chat_history, gr.update(value=\"\"), video_frame\n",
    "    \n",
    "    def video_frame_update():\n",
    "        \"\"\"Update the video frame regularly\"\"\"\n",
    "        if not assistant.video_interface.is_running:\n",
    "            blank_frame = np.zeros((240, 640, 3), dtype=np.uint8)\n",
    "            cv2.putText(blank_frame, \"Start video call to see avatars\", (180, 120), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            return blank_frame\n",
    "            \n",
    "        return assistant.video_interface.generate_combined_frame()\n",
    "    \n",
    "    # Custom CSS for styling\n",
    "    custom_css = \"\"\"\n",
    "    .gradio-container {\n",
    "        background: linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab);\n",
    "        background_size: 400% 400%;\n",
    "        animation: gradient 15s ease infinite;\n",
    "    }\n",
    "    @keyframes gradient {\n",
    "        0% { background-position: 0% 50%; }\n",
    "        50% { background-position: 100% 50%; }\n",
    "        100% { background-position: 0% 50%; }\n",
    "    }\n",
    "    .gradio-interface {\n",
    "        background: rgba(255, 255, 255, 0.9);\n",
    "        border-radius: 15px;\n",
    "        padding: 20px;\n",
    "        box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);\n",
    "        backdrop-filter: blur(5px);\n",
    "        -webkit-backdrop-filter: blur(5px);\n",
    "    }\n",
    "    .video-interface {\n",
    "        background: rgba(255, 255, 255, 0.95);\n",
    "        border-radius: 15px;\n",
    "        padding: 15px;\n",
    "        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.15);\n",
    "    }\n",
    "    .gradio-feedback {\n",
    "        background: rgba(245, 245, 245, 0.9);\n",
    "        border-radius: 10px;\n",
    "        padding: 15px;\n",
    "    }\n",
    "    .video-container {\n",
    "        display: flex;\n",
    "        flex-direction: column;\n",
    "        width: 100%;\n",
    "    }\n",
    "    .chat-container {\n",
    "        display: flex;\n",
    "        flex-direction: column;\n",
    "        flex-grow: 1;\n",
    "        min-height: 300px;\n",
    "        overflow-y: auto;\n",
    "        padding: 10px;\n",
    "        background: rgba(240, 240, 240, 0.7);\n",
    "        border-radius: 8px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "    .language-info {\n",
    "        background: rgba(255, 255, 255, 0.7);\n",
    "        padding: 5px 10px;\n",
    "        border-radius: 5px;\n",
    "        display: inline-block;\n",
    "        margin-top: 5px;\n",
    "        font-size: 0.9em;\n",
    "        color: #555;\n",
    "    }\n",
    "    .cloud-notice {\n",
    "        background: rgba(255, 200, 0, 0.2);\n",
    "        border: 1px solid #FFB800;\n",
    "        padding: 8px;\n",
    "        border-radius: 5px;\n",
    "        margin-bottom: 10px;\n",
    "        font-size: 0.9em;\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"Loubby Navigation Assistant\", css=custom_css) as interface:\n",
    "        gr.Markdown(\"\"\"\n",
    "        <div style=\"text-align: center;\">\n",
    "            <h1 style=\"color: white;\">Loubby AI Navigation Assistant</h1>\n",
    "            <p style=\"color: white;\">Your personal guide to navigating the Loubby platform</p>\n",
    "        </div>\n",
    "        <div class=\"cloud-notice\">\n",
    "            <strong>Cloud Environment Notice:</strong> Running in cloud mode with virtual video avatars instead of camera input.\n",
    "        </div>\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            with gr.Tab(\"Text Interface\", elem_classes=\"gradio-interface\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=3):\n",
    "                        text_input = gr.Textbox(label=\"Your Question\", placeholder=\"How do I upload my resume?\", lines=2)\n",
    "                        \n",
    "                        language_info = gr.Markdown(\"Language will be detected automatically\", elem_classes=\"language-info\")\n",
    "                        voice_checkbox = gr.Checkbox(label=\"Enable Voice Output\", value=False)\n",
    "                        text_submit = gr.Button(\"Ask\", variant=\"primary\")\n",
    "                    \n",
    "                    with gr.Column(scale=4):\n",
    "                        text_output = gr.Textbox(label=\"Response\", lines=8)\n",
    "                        sources_output = gr.Textbox(label=\"Sources\")\n",
    "                        audio_output = gr.Audio(label=\"Voice Response\", visible=False)\n",
    "                \n",
    "                text_submit.click(\n",
    "                    process_text_query,\n",
    "                    inputs=[text_input, voice_checkbox],\n",
    "                    outputs=[text_output, sources_output, audio_output, language_info]\n",
    "                )\n",
    "                \n",
    "                # Also allow pressing Enter to submit\n",
    "                text_input.submit(\n",
    "                    process_text_query,\n",
    "                    inputs=[text_input, voice_checkbox],\n",
    "                    outputs=[text_output, sources_output, audio_output, language_info]\n",
    "                )\n",
    "                \n",
    "                voice_checkbox.change(\n",
    "                    lambda x: gr.Audio.update(visible=x),\n",
    "                    inputs=voice_checkbox,\n",
    "                    outputs=audio_output\n",
    "                )\n",
    "            \n",
    "            with gr.Tab(\"Voice Interface\", elem_classes=\"gradio-interface\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        voice_info = gr.Markdown(\"Language will be detected automatically\", elem_classes=\"language-info\")\n",
    "                        voice_input = gr.Audio(label=\"Speak Your Question\", type=\"filepath\")\n",
    "                        voice_button = gr.Button(\"Submit\", variant=\"primary\")\n",
    "                    with gr.Column():\n",
    "                        voice_query_display = gr.Textbox(label=\"Your Question\")\n",
    "                        voice_response = gr.Textbox(label=\"Response\", lines=8)\n",
    "                        voice_sources = gr.Textbox(label=\"Sources\")\n",
    "                        voice_audio_output = gr.Audio(label=\"Voice Response\")\n",
    "                \n",
    "                voice_button.click(\n",
    "                    process_voice_query,\n",
    "                    inputs=[voice_input],\n",
    "                    outputs=[voice_query_display, voice_response, voice_sources, voice_audio_output, voice_info]\n",
    "                )\n",
    "            \n",
    "            with gr.Tab(\"Video Communication\", elem_classes=\"video-interface\"):\n",
    "                # Initialize chat history state\n",
    "                chat_history = gr.State([])\n",
    "                \n",
    "                with gr.Row():\n",
    "                    # Video feed display with combined user and assistant views\n",
    "                    video_display = gr.Image(label=\"Video Call\", elem_id=\"video-display\")\n",
    "                    \n",
    "                    # Controls and chat area\n",
    "                    with gr.Column():\n",
    "                        video_call_status = gr.Markdown(\"Click 'Start Video Call' to begin communicating with the assistant.\")\n",
    "                        \n",
    "                        # Start and end call buttons\n",
    "                        with gr.Row():\n",
    "                            start_button = gr.Button(\"Start Video Call\", variant=\"primary\")\n",
    "                            end_button = gr.Button(\"End Video Call\", variant=\"secondary\")\n",
    "                        \n",
    "                        # Chat display\n",
    "                        chat_display = gr.Chatbot(label=\"Conversation\", elem_classes=\"chat-container\")\n",
    "                        \n",
    "                        # Chat input during video call\n",
    "                        with gr.Row():\n",
    "                            video_chat_input = gr.Textbox(\n",
    "                                label=\"Type a message\", \n",
    "                                placeholder=\"Ask a question about the Loubby platform...\",\n",
    "                                lines=2\n",
    "                            )\n",
    "                            video_chat_button = gr.Button(\"Send\", variant=\"primary\")\n",
    "                \n",
    "                # Connect everything\n",
    "                start_button.click(\n",
    "                    start_video_call,\n",
    "                    inputs=None,\n",
    "                    outputs=video_call_status\n",
    "                )\n",
    "                \n",
    "                end_button.click(\n",
    "                    end_video_call,\n",
    "                    inputs=None,\n",
    "                    outputs=video_call_status\n",
    "                )\n",
    "                \n",
    "                # Handle sending messages in video chat\n",
    "                video_chat_button.click(\n",
    "                    video_chat_message,\n",
    "                    inputs=[video_chat_input, chat_history],\n",
    "                    outputs=[chat_display, video_chat_input, video_display]\n",
    "                ).then(\n",
    "                    lambda x: x,\n",
    "                    inputs=chat_display,\n",
    "                    outputs=chat_history\n",
    "                )\n",
    "                \n",
    "                # Also allow pressing Enter to send message\n",
    "                video_chat_input.submit(\n",
    "                    video_chat_message,\n",
    "                    inputs=[video_chat_input, chat_history],\n",
    "                    outputs=[chat_display, video_chat_input, video_display]\n",
    "                ).then(\n",
    "                    lambda x: x,\n",
    "                    inputs=chat_display,\n",
    "                    outputs=chat_history\n",
    "                )\n",
    "                \n",
    "                # Update video frame every 0.5 seconds\n",
    "                #gr.on(\n",
    "                    #\"load\",\n",
    "                    #lambda: gr.update(value=video_frame_update()),\n",
    "                    #None,\n",
    "                    #video_display,\n",
    "                    #every=0.5\n",
    "                #)\n",
    "            \n",
    "            with gr.Tab(\"Feedback\", elem_classes=\"gradio-feedback\"):\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        feedback_query = gr.Textbox(label=\"Question\", lines=2)\n",
    "                        feedback_response = gr.Textbox(label=\"Response\", lines=4)\n",
    "                        feedback_rating = gr.Slider(minimum=1, maximum=5, step=1, value=3, label=\"Rating\")\n",
    "                        feedback_comment = gr.Textbox(label=\"Comment (Optional)\", lines=2)\n",
    "                        feedback_submit = gr.Button(\"Submit Feedback\", variant=\"primary\")\n",
    "                        feedback_result = gr.Textbox(label=\"Result\")\n",
    "                    with gr.Column():\n",
    "                        stats_button = gr.Button(\"Show Feedback Statistics\", variant=\"secondary\")\n",
    "                        stats_display = gr.Textbox(label=\"Statistics\")\n",
    "                \n",
    "                feedback_submit.click(\n",
    "                    submit_feedback,\n",
    "                    inputs=[feedback_query, feedback_response, feedback_rating, feedback_comment],\n",
    "                    outputs=[feedback_result]\n",
    "                )\n",
    "                \n",
    "                stats_button.click(\n",
    "                    show_statistics,\n",
    "                    inputs=None,\n",
    "                    outputs=[stats_display]\n",
    "                )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "def run_main():\n",
    "    \"\"\"Function to run the main assistant\"\"\"\n",
    "    # Initialize the assistant\n",
    "    assistant = LoubbyAssistant()\n",
    "    \n",
    "    # Create and launch the Gradio interface\n",
    "    interface = create_interface(assistant)\n",
    "    interface.launch(share=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8715b7b3-d462-47b1-88b0-c73a7994afc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
